"""
–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç–≤–µ—Ç–æ–≤ AI –¥–ª—è Yandex Cloud (YandexGPT).

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Yandex Cloud AI —Å–µ—Ä–≤–∏—Å—ã (YandexGPT Lite, SpeechKit STT, Vision OCR).
–°–æ–±–ª—é–¥–∞–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É SOLID.
"""

import random
import re
from abc import ABC, abstractmethod

from loguru import logger

from bot.config import settings
from bot.services.knowledge_service import get_knowledge_service
from bot.services.prompt_builder import get_prompt_builder
from bot.services.rag import ContextCompressor
from bot.services.yandex_cloud_service import get_yandex_cloud_service


def add_random_engagement_question(response: str) -> str:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π –≤–æ–ø—Ä–æ—Å –¥–ª—è –≤–æ–≤–ª–µ—á–µ–Ω–∏—è –≤ –∫–æ–Ω–µ—Ü –æ—Ç–≤–µ—Ç–∞.

    –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –í–æ–ø—Ä–æ—Å –í–°–ï–ì–î–ê –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç–¥–µ–ª–µ–Ω –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π –æ—Ç –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.

    Args:
        response: –ò—Å—Ö–æ–¥–Ω—ã–π –æ—Ç–≤–µ—Ç AI

    Returns:
        str: –û—Ç–≤–µ—Ç —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º —Å–ª—É—á–∞–π–Ω—ã–º –≤–æ–ø—Ä–æ—Å–æ–º (–æ—Ç–¥–µ–ª–µ–Ω–Ω—ã–º –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π)
    """
    if not response or not response.strip():
        return response

    # –í–∞—Ä–∏–∞–Ω—Ç—ã –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –≤–æ–≤–ª–µ—á–µ–Ω–∏—è
    engagement_questions = [
        "–ü–æ–Ω—è—Ç–Ω–æ? –ú–æ–≥—É –æ–±—ä—è—Å–Ω–∏—Ç—å –ø–æ–¥—Ä–æ–±–Ω–µ–µ?",
        "–û–±—ä—è—Å–Ω–∏—Ç—å –ø–æ–¥—Ä–æ–±–Ω–µ–µ?",
        "–°–ø—Ä–æ—Å–∏ –º–µ–Ω—è –µ—â—ë —á—Ç–æ-–Ω–∏–±—É–¥—å, –º–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è —Å —Ç–æ–±–æ–π –æ–±—â–∞—Ç—å—Å—è!",
        "–•–æ—á–µ—à—å, –æ–±—ä—è—Å–Ω—é –ø–æ–¥—Ä–æ–±–Ω–µ–µ...",
        "–ï—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã –ø–æ—Å–ª–æ–∂–Ω–µ–µ?",
    ]

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ –≤–æ–ø—Ä–æ—Å–∞ –≤ –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞ (–±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
    response_lower = response.lower().strip()
    question_indicators = [
        "–ø–æ–Ω—è—Ç–Ω–æ?",
        "–æ–±—ä—è—Å–Ω–∏—Ç—å –ø–æ–¥—Ä–æ–±–Ω–µ–µ",
        "—Å–ø—Ä–æ—Å–∏ –º–µ–Ω—è",
        "–µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã",
        "—Ö–æ—á–µ—à—å, –æ–±—ä—è—Å–Ω—é",
        "—Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω–µ–µ",
        "–ø–æ–¥—Ä–æ–±–Ω–µ–µ?",
    ]

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 150 —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –≤–æ–ø—Ä–æ—Å–∞ (–±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
    last_part = response_lower[-150:] if len(response_lower) > 150 else response_lower
    has_existing_question = any(indicator in last_part for indicator in question_indicators)

    if has_existing_question:
        # –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —É–∂–µ –µ—Å—Ç—å, –ø—Ä–æ—Å—Ç–æ —É–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –æ–Ω –æ—Ç–¥–µ–ª–µ–Ω –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π
        response_stripped = response.strip()
        if not response_stripped.endswith("\n\n") and "\n\n" not in response_stripped[-50:]:
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –ø–µ—Ä–µ–¥ –ø–æ—Å–ª–µ–¥–Ω–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
            lines = response_stripped.split("\n")
            if len(lines) > 1 and lines[-1].strip():
                # –ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–æ–∫–∞ –Ω–µ –ø—É—Å—Ç–∞—è, –¥–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
                return "\n".join(lines[:-1]) + "\n\n" + lines[-1]
        return response

    # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –≤–æ–ø—Ä–æ—Å
    random_question = random.choice(engagement_questions)

    # –í–°–ï–ì–î–ê –æ—Ç–¥–µ–ª—è–µ–º –≤–æ–ø—Ä–æ—Å –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π –æ—Ç –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    response_stripped = response.strip()

    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –≤ –∫–æ–Ω—Ü–µ
    while response_stripped.endswith("\n"):
        response_stripped = response_stripped.rstrip("\n")

    # –î–æ–±–∞–≤–ª—è–µ–º –≤–æ–ø—Ä–æ—Å —Å –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π –ø–µ—Ä–µ–¥ –Ω–∏–º
    return f"{response_stripped}\n\n{random_question}"


def _normalize_for_dedup(s: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: —É–±–∏—Ä–∞–µ–º ** –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã, —á—Ç–æ–±—ã –¥—É–±–ª–∏ –Ω–µ —Ä–∞–∑–ª–∏—á–∞–ª–∏—Å—å –∏–∑-–∑–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
    if not s:
        return ""
    s = re.sub(r"\*\*", "", s.lower().strip())
    return re.sub(r"\s+", " ", s)


def _remove_duplicate_long_substrings(text: str, min_len: int = 70) -> str:
    """
    –£–¥–∞–ª—è–µ—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –¥–ª–∏–Ω–Ω—ã–µ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ (–∞—Ä—Ç–µ—Ñ–∞–∫—Ç —Å—Ç—Ä–∏–º–∏–Ω–≥–∞: –≤—Å—Ç–∞–≤–∫–∞ –±–ª–æ–∫–∞ –ø–æ–≤—Ç–æ—Ä–Ω–æ).
    –£–±–∏—Ä–∞–µ—Ç –≤—Ç–æ—Ä–æ–µ –∏ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è –±–ª–æ–∫–∞ –¥–ª–∏–Ω–æ–π min_len+ —Å–∏–º–≤–æ–ª–æ–≤.
    """
    if not text or len(text) < min_len * 2:
        return text
    result = text
    changed = True
    while changed:
        changed = False
        for length in range(min(len(result) // 2, 200), min_len - 1, -10):
            for i in range(len(result) - length):
                sub = result[i : i + length]
                if sub.strip() and len(sub.strip()) >= min_len:
                    j = result.find(sub, i + 1)
                    if j != -1:
                        result = result[:j] + result[j + length :]
                        changed = True
                        break
            if changed:
                break
    return result


def remove_duplicate_text(text: str, min_length: int = 20) -> str:
    """
    –£–¥–∞–ª—è–µ—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Ç–µ–∫—Å—Ç–∞ (–¥—É–±–ª–∏–∫–∞—Ç—ã).
    –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è –≤—Å–µ—Ö –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π.

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        min_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç

    Returns:
        str: –¢–µ–∫—Å—Ç –±–µ–∑ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
    """
    if not text or len(text) < min_length * 2:
        return text

    # –®–∞–≥ 1: –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è –ª–∏ –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Ü–µ–ª–∏–∫–æ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑
    text_len = len(text)
    if text_len > min_length * 3:
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ 3 —á–∞—Å—Ç–∏ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º
        part_size = text_len // 3
        parts = [text[i : i + part_size] for i in range(0, text_len, part_size)]
        if len(parts) >= 2:
            normalized_parts = [re.sub(r"\s+", " ", p.lower().strip()) for p in parts[:3]]
            # –ï—Å–ª–∏ –≤—Å–µ —á–∞—Å—Ç–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é
            if len(normalized_parts) >= 2 and all(
                p == normalized_parts[0] for p in normalized_parts[1:] if len(p) >= min_length
            ):
                return parts[0].strip()

    # –®–∞–≥ 2: –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏ (–ø–æ –ø–µ—Ä–µ–Ω–æ—Å–∞–º)
    lines = [line.strip() for line in text.split("\n") if line.strip()]

    if len(lines) < 2 and len(text) <= 300:
        return text

    # –®–∞–≥ 3: –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—Ç—Ä–æ–∫
    # –ù–û —Å–æ—Ö—Ä–∞–Ω—è–µ–º markdown –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –º–∞—Ä–∫–µ—Ä—ã —Å–ø–∏—Å–∫–æ–≤!
    seen_lines = set()
    unique_lines = []

    def _content_for_dedup(s: str) -> str:
        """–°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ –Ω–æ–º–µ—Ä–∞/–º–∞—Ä–∫–µ—Ä–∞ —Å–ø–∏—Å–∫–∞ ‚Äî –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏."""
        t = s.strip()
        if re.match(r"^\d+\.\s*", t):
            t = re.sub(r"^\d+\.\s*", "", t)
        if t.startswith("- ") or t.startswith("* ") or t.startswith("‚Ä¢ "):
            t = t[2:].strip()
        return _normalize_for_dedup(t)

    for line in lines:
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ (#) –Ω–µ –¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        if line.strip().startswith("#"):
            unique_lines.append(line)
            continue

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: –¥–ª—è —Å–ø–∏—Å–∫–æ–≤ (1., -, *) ‚Äî –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É –±–µ–∑ –º–∞—Ä–∫–µ—Ä–∞
        normalized = _content_for_dedup(line)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É (–ª–æ–≤–∏–º –ø–æ–≤—Ç–æ—Ä—ã ¬´1. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ‚Ä¶¬ª / ¬´1. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ‚Ä¶¬ª)
        if len(normalized) >= min_length:
            if normalized not in seen_lines:
                seen_lines.add(normalized)
                unique_lines.append(line)
        else:
            if line not in unique_lines:
                unique_lines.append(line)

    result = "\n".join(unique_lines)

    # –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –±–ª–æ–∫–∏ (–Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –ø–æ–¥—Ä—è–¥)
    if len(unique_lines) >= 4:
        # –ò—â–µ–º –±–ª–æ–∫–∏ –∏–∑ 2-5 —Å—Ç—Ä–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–≤—Ç–æ—Ä—è—é—Ç—Å—è
        seen_blocks = set()
        final_lines = []
        i = 0

        while i < len(unique_lines):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–ª–æ–∫–∏ —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã
            found_duplicate = False
            for block_len in range(5, 1, -1):  # –û—Ç 5 –¥–æ 2 —Å—Ç—Ä–æ–∫
                if i + block_len > len(unique_lines):
                    continue

                block = "\n".join(unique_lines[i : i + block_len])
                normalized_block = _normalize_for_dedup(block)

                if len(normalized_block) >= min_length * 2:
                    if normalized_block in seen_blocks:
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–µ—Å—å –±–ª–æ–∫
                        i += block_len
                        found_duplicate = True
                        break
                    else:
                        seen_blocks.add(normalized_block)

            if not found_duplicate:
                final_lines.append(unique_lines[i])
                i += 1

        result = "\n".join(final_lines)

    # –®–∞–≥ 4.5: –û–¥–∏–Ω –¥–ª–∏–Ω–Ω—ã–π –∞–±–∑–∞—Ü –±–µ–∑ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ (—Ç–∏–ø–∏—á–Ω–æ ¬´—Ñ–æ—Ä–º—É–ª—ã¬ª) ‚Äî —Ä–µ–∂–µ–º –ø–æ –≥—Ä–∞–Ω–∏—Ü–∞–º –±–ª–æ–∫–æ–≤ –∏ –¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º
    if len(result) > 300 and "\n\n" not in result:
        parts = re.split(
            r"(?=–§–æ—Ä–º—É–ª–∞ –¥–ª—è|–î–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á|–í–æ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑ –Ω–∏—Ö|–≥–¥–µ:)",
            result,
            flags=re.IGNORECASE,
        )
        if len(parts) >= 2:
            seen_parts = set()
            unique_parts = []
            for seg in parts:
                seg = seg.strip()
                if not seg or len(seg) < 50:
                    if seg:
                        unique_parts.append(seg)
                    continue
                norm = _normalize_for_dedup(seg)
                if len(norm) < 80:
                    unique_parts.append(seg)
                    continue
                is_dup = norm in seen_parts
                if not is_dup:
                    for seen in seen_parts:
                        if len(seen) < 80:
                            continue
                        w_new = set(norm.split())
                        w_seen = set(seen.split())
                        if w_new and w_seen:
                            sim = len(w_new & w_seen) / max(len(w_new), len(w_seen))
                            if sim > 0.55:
                                is_dup = True
                                break
                if not is_dup:
                    seen_parts.add(norm)
                    unique_parts.append(seg)
            if unique_parts:
                result = "\n\n".join(unique_parts)

    # –®–∞–≥ 5: –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∞–±–∑–∞—Ü—ã (–ø–æ—Ö–æ–∂–µ—Å—Ç—å 50%, –ø–æ–¥—Å—Ç—Ä–æ–∫–∞, –±–ª–æ–∫ —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º)
    raw_paragraphs = [p.strip() for p in result.split("\n\n") if p.strip()]
    if len(raw_paragraphs) == 1 and "\n" in result:
        paragraphs = [p.strip() for p in result.split("\n") if len(p.strip()) > 30]
    else:
        paragraphs = raw_paragraphs
    if len(paragraphs) >= 2:
        seen_paragraphs = set()
        unique_paragraphs = []
        normalized_list = [_normalize_for_dedup(p) for p in paragraphs]
        min_para_len = 25

        for idx, paragraph in enumerate(paragraphs):
            normalized_para = normalized_list[idx]
            words_new = set(normalized_para.split())
            is_duplicate = False
            for seen_para in seen_paragraphs:
                words_seen = set(seen_para.split())
                if len(words_new) > 0 and len(words_seen) > 0:
                    common = len(words_new & words_seen)
                    similarity = common / max(len(words_new), len(words_seen))
                    # –î–ª–∏–Ω–Ω—ã–µ –ø–æ—á—Ç–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∞–±–∑–∞—Ü—ã (–ø–æ–≤—Ç–æ—Ä –≤–≤–æ–¥–Ω–æ–π —Ñ—Ä–∞–∑—ã —Å–ø–∏—Å–∫–∞ –∏ —Ç.–ø.)
                    long_both = min(len(words_new), len(words_seen)) > 20
                    if similarity > 0.50 or (long_both and similarity > 0.80):
                        is_duplicate = True
                        break
                if (
                    len(normalized_para) >= min_para_len
                    and len(seen_para) >= min_para_len
                    and (normalized_para in seen_para or seen_para in normalized_para)
                ):
                    is_duplicate = True
                    break

            if not is_duplicate:
                seen_paragraphs.add(normalized_para)
                unique_paragraphs.append(paragraph)

        # –ê–±–∑–∞—Ü—ã —Å –ª–∏—à–Ω–∏–º –ø—Ä–µ—Ñ–∏–∫—Å–æ–º (1‚Äì4 —Å–ª–æ–≤–∞): ¬´–ö–Ω–∏–≥–∞ –í–æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ‚Ä¶¬ª ‚Äî —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç
        if len(unique_paragraphs) >= 2:
            norm_unique = [_normalize_for_dedup(p) for p in unique_paragraphs]
            to_remove = set()
            for j in range(len(unique_paragraphs)):
                wj = norm_unique[j].split()
                for i in range(len(unique_paragraphs)):
                    if i == j or i in to_remove or j in to_remove:
                        continue
                    wi = norm_unique[i].split()
                    if len(wi) < 10:
                        continue
                    for prefix_len in range(1, min(5, len(wj))):
                        if wj[prefix_len:] == wi:
                            to_remove.add(j)
                            break
            unique_paragraphs = [p for k, p in enumerate(unique_paragraphs) if k not in to_remove]
        result = "\n\n".join(unique_paragraphs)

    # –®–∞–≥ 6: –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–≤–∫–ª—é—á–∞—è –ø–æ—Ö–æ–∂–∏–µ –ø–æ —Å–ª–æ–≤–∞–º >70%)
    sentences = re.split(r"([.!?]\s+)", result)
    if len(sentences) >= 4:
        seen_normalized = set()
        unique_sentences = []
        sent_min_len = 40

        i = 0
        while i < len(sentences) - 1:
            sentence = sentences[i] + (sentences[i + 1] if i + 1 < len(sentences) else "")
            normalized_sent = _normalize_for_dedup(sentence)
            if len(normalized_sent) < sent_min_len:
                unique_sentences.append(sentence)
                i += 2
                continue
            is_dup = normalized_sent in seen_normalized
            if not is_dup:
                for seen in seen_normalized:
                    if len(seen) < sent_min_len:
                        continue
                    w_new = set(normalized_sent.split())
                    w_seen = set(seen.split())
                    if w_new and w_seen:
                        sim = len(w_new & w_seen) / max(len(w_new), len(w_seen))
                        if sim > 0.7:
                            is_dup = True
                            break
            if not is_dup:
                seen_normalized.add(normalized_sent)
                unique_sentences.append(sentence)
            i += 2

        result = "".join(unique_sentences)

    # –®–∞–≥ 7: –°–û–•–†–ê–ù–Ø–ï–ú markdown —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –æ—Ç–≤–µ—Ç–æ–≤!
    # –ù–ï –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏ –ù–ï —É–¥–∞–ª—è–µ–º:
    # - ### –∑–∞–≥–æ–ª–æ–≤–∫–∏ (—Ñ—Ä–æ–Ω—Ç–µ–Ω–¥ —É–º–µ–µ—Ç –∏—Ö —Ä–µ–Ω–¥–µ—Ä–∏—Ç—å)
    # - **–∂–∏—Ä–Ω—ã–π** (—Ñ—Ä–æ–Ω—Ç–µ–Ω–¥ —É–º–µ–µ—Ç)
    # - *–∫—É—Ä—Å–∏–≤* (—Ñ—Ä–æ–Ω—Ç–µ–Ω–¥ —É–º–µ–µ—Ç)
    # - —Å–ø–∏—Å–∫–∏ —Å "-" –∏–ª–∏ "*" (—Ñ—Ä–æ–Ω—Ç–µ–Ω–¥ —É–º–µ–µ—Ç)
    # - –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ "1. 2. 3." (—Ñ—Ä–æ–Ω—Ç–µ–Ω–¥ —É–º–µ–µ—Ç)
    #
    # –§—Ä–æ–Ω—Ç–µ–Ω–¥ (AIChat.tsx) —É–∂–µ –∏–º–µ–µ—Ç –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ markdown!

    # –®–∞–≥ 8: –§–∏–Ω–∞–ª—å–Ω–∞—è –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è - —É–¥–∞–ª—è–µ–º —Ç–æ—á–Ω—ã–µ –ø–æ–≤—Ç–æ—Ä—ã
    # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –≤—Å–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º –∏ —É–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
    final_chunks = re.split(r"(\n\n|\n|\.  |\.(?=\s+[–ê-–ØA-Z]))", result)
    deduplicated_chunks = []
    prev_chunk_normalized = None

    for chunk in final_chunks:
        if not chunk or chunk in ("\n", "\n\n", ".  "):
            deduplicated_chunks.append(chunk)
            continue

        chunk_normalized = _normalize_for_dedup(chunk)
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –¥—É–±–ª–∏–∫–∞—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ (—Å —É—á—ë—Ç–æ–º **)
        if chunk_normalized and chunk_normalized != prev_chunk_normalized:
            deduplicated_chunks.append(chunk)
            prev_chunk_normalized = chunk_normalized
        elif not chunk_normalized:
            deduplicated_chunks.append(chunk)

    result = "".join(deduplicated_chunks)

    return result.strip() if result.strip() else text.strip()


def normalize_bold_spacing(text: str) -> str:
    """–í—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª –ø–µ—Ä–µ–¥ –∏ –ø–æ—Å–ª–µ ** –º–µ–∂–¥—É –±—É–∫–≤–∞–º–∏: —Å–ª–æ–≤–æ**—Ç–µ—Ä–º–∏–Ω** ‚Üí —Å–ª–æ–≤–æ **—Ç–µ—Ä–º–∏–Ω**."""
    if not text or "**" not in text:
        return text
    text = re.sub(r"(\w)\*\*", r"\1 **", text)
    text = re.sub(r"\*\*(\w)", r"** \1", text)
    return text


def fix_glued_words(text: str) -> str:
    """
    –ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç —Å–∫–ª–µ–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞: –£–ü—Ä–∏–≤–µ—Ç, —à–µ–ü—Ä–∏–≤–µ—Ç, –≤–µ—Ä—à–∏–Ω–≤–µ—Ä—à–∏–Ω–∞, –†–µ–∫–∞–∫–∏—Ö –∏ —Ç.–ø.
    """
    if not text or len(text) < 4:
        return text
    # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–∫–ª–µ–π–∫–∏ (–∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –º–æ–¥–µ–ª–∏)
    glued_fixes = [
        (r"\b–£–ü—Ä–∏–≤–µ—Ç\b", "–ü—Ä–∏–≤–µ—Ç"),
        (r"\b—à–µ–ü—Ä–∏–≤–µ—Ç\b", "–ü—Ä–∏–≤–µ—Ç"),
        (r"\b–£–ü—Ä–µ–∑–µ–Ω—Ç\b", "–ü—Ä–µ–∑–µ–Ω—Ç"),
        (r"\b–∏–ü—Ä–µ–∑–µ–Ω—Ç\b", "–∏ –ü—Ä–µ–∑–µ–Ω—Ç"),
        (r"\b—à–µ–≠—Ç–æ\b", "–≠—Ç–æ"),
        (r"–≤–µ—Ä—à–∏–Ω–≤–µ—Ä—à–∏–Ω–∞", "–≤–µ—Ä—à–∏–Ω–∞"),
        (r"—Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ–µ—Ä–æ–º", "—Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –∑–∞–≥–æ–≤–æ—Ä–∞"),
        (r"–†–µ–∫–∞–∫–∏—Ö\b", "Re. –ü—Ä–∏ –∫–∞–∫–∏—Ö"),
    ]
    for pattern, replacement in glued_fixes:
        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
    # –ü–æ–≤—Ç–æ—Ä —Å–ª–æ–≤–∞ –ø–æ–¥—Ä—è–¥ –±–µ–∑ –ø—Ä–æ–±–µ–ª–∞ (–≤–µ—Ä—à–∏–Ω–≤–µ—Ä—à–∏–Ω–∞ —É–∂–µ –≤—ã—à–µ; –æ–±—â–∏–π —Å–ª—É—á–∞–π –¥–ª—è 3+ –±—É–∫–≤)
    text = re.sub(r"\b(\w{3,})\1\b", r"\1", text)
    # –û–±—â–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω: –æ–¥–Ω–∞ –±—É–∫–≤–∞/—Å–ª–æ–≥ + –∑–∞–≥–ª–∞–≤–Ω–æ–µ —Å–ª–æ–≤–æ (–ü—Ä–∏–≤–µ—Ç, –ü—Ä–µ–∑–µ–Ω—Ç, –≠—Ç–æ) ‚Üí –ø—Ä–æ–±–µ–ª + —Å–ª–æ–≤–æ
    text = re.sub(r"([–∞-—è—ëa-z])([–ê-–Ø–ÅA-Z][–∞-—è—ëa-z]{2,})", r"\1 \2", text)
    return text


def clean_ai_response(text: str) -> str:
    """
    –û—á–∏—â–∞–µ—Ç –æ—Ç–≤–µ—Ç AI –æ—Ç LaTeX, —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (>, <) –∏ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è.
    –ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —É–º–Ω–æ–∂–µ–Ω–∏—è.
    –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –ø–µ—Ä–≤—ã–µ —Å–ª–æ–≤–∞ –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ.
    –ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç —Å–∫–ª–µ–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ (–£–ü—Ä–∏–≤–µ—Ç, –∏–ü—Ä–µ–∑–µ–Ω—Ç).
    """
    if not text:
        return text

    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Å–∫–ª–µ–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –Ω–∞—á–∞–ª–µ
    text = fix_glued_words(text)

    # –£–¥–∞–ª—è–µ–º –≤—Å—Ç–∞–≤–∫–∏ –≤ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö (–∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –º–æ–¥–µ–ª–∏)
    text = re.sub(r"\[–ü—Ä–∏–ª–æ–∂–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ[^\]]*\]", "", text, flags=re.IGNORECASE)
    text = re.sub(r"\[–î–∞–π[^\]]*\]", "", text, flags=re.IGNORECASE)
    text = re.sub(r"\[(?:–ö—Ç–æ —Ç–∞–∫–æ–π|–ß—Ç–æ —Ç–∞–∫–æ–µ|–ö—Ç–æ —Ç–∞–∫–∞—è)[^\]]*\]", "", text, flags=re.IGNORECASE)
    text = re.sub(r"\[[^\]]{15,}\]", "", text)  # –¥–ª–∏–Ω–Ω—ã–µ —Å–∫–æ–±–∫–∏ ‚Äî –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏/–∑–∞–≥–æ–ª–æ–≤–∫–∏

    # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –¥–ª–∏–Ω–Ω—ã–µ –±–ª–æ–∫–∏ (—Å—Ç—Ä–∏–º–∏–Ω–≥ –∏–Ω–æ–≥–¥–∞ –≤—Å—Ç–∞–≤–ª—è–µ—Ç –±–ª–æ–∫ –¥–≤–∞–∂–¥—ã)
    text = _remove_duplicate_long_substrings(text, min_len=70)

    # LaTeX –≤ —Ñ–æ—Ä–º—É–ª–∞—Ö ‚Üí —Ç–∏–ø–æ–≥—Ä–∞—Ñ—Å–∫–∏–π —Å—Ç–∏–ª—å: —É–±–∏—Ä–∞–µ–º \quad, \text{}, \left/\right
    text = re.sub(r"\\quad\s*", " ", text)
    text = re.sub(r"\\text\s*\{([^}]*)\}", r"\1", text)
    text = re.sub(r"\\left\s*\(\s*", "(", text)
    text = re.sub(r"\s*\\right\s*\)", ")", text)
    text = re.sub(r"\\left\s*\[\s*", "[", text)
    text = re.sub(r"\s*\\right\s*\]", "]", text)
    # –ë–µ–∑ —Å–ª—ç—à–∞: left( / right) ‚Üí ( / )
    text = re.sub(r"\bleft\s*\(\s*", "(", text, flags=re.IGNORECASE)
    text = re.sub(r"\s*right\s*\)", ")", text, flags=re.IGNORECASE)

    # –ü—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥ ** –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∂–∏—Ä–Ω–æ–≥–æ
    text = normalize_bold_spacing(text)

    # –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–µ—Ä–≤–∞—è –±—É–∫–≤–∞ –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–í–í –∫–∞–∫–æ–º -> –í –∫–∞–∫–æ–º)
    text = re.sub(r"(^|[\n.]\s*)([–ê-–Ø–∞-—èA-Za-z])\2(\s)", r"\1\2\3", text)

    # –ü–æ–≤—Ç–æ—Ä –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ —Å–ª–æ–≤–∞ –ø–æ–¥—Ä—è–¥ (—Ñ–∞–∫—Ç—ã —Ñ–∞–∫—Ç—ã ‚Üí —Ñ–∞–∫—Ç—ã)
    text = re.sub(r"\b(\w{4,})\s+\1\b", r"\1", text)

    # –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –ø–µ—Ä–≤—ã–µ —Å–ª–æ–≤–∞
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 1-5 —Å–ª–æ–≤ –Ω–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
    words = text.split()
    if len(words) >= 2:
        # –®–∞–≥ 1: –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –¥—É–±–ª–∏—Ä—É–µ—Ç—Å—è –ª–∏ –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ —Ü–µ–ª–∏–∫–æ–º
        first_word = words[0].strip()
        # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        first_word_clean = re.sub(r"[^\w]", "", first_word.lower())

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –¥—É–±–ª–∏—Ä—É–µ—Ç—Å—è –ª–∏ –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ –≤ —Å–æ—Å—Ç–∞–≤–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–ñ–∏–≤—É–ñ–∏–≤—É")
        if len(first_word_clean) >= 4 and len(first_word_clean) % 2 == 0:
            half_len = len(first_word_clean) // 2
            first_half = first_word_clean[:half_len]
            second_half = first_word_clean[half_len:]
            if first_half == second_half:
                # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤–∞
                text = first_word[:half_len] + " " + " ".join(words[1:])
                words = text.split()

        # –®–∞–≥ 2: –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –¥—É–±–ª–∏—Ä—É–µ—Ç—Å—è –ª–∏ –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ —Ü–µ–ª–∏–∫–æ–º –≤–æ –≤—Ç–æ—Ä–æ–º —Å–ª–æ–≤–µ
        if len(words) >= 2:
            second_word_clean = re.sub(r"[^\w]", "", words[1].lower())
            if first_word_clean == second_word_clean:
                # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç –≤—Ç–æ—Ä–æ–≥–æ —Å–ª–æ–≤–∞
                text = " ".join([words[0]] + words[2:])
                words = text.split()

        # –®–∞–≥ 3: –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–≤—ã—Ö 2-5 —Å–ª–æ–≤ (–±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ)
        for word_count in range(5, 1, -1):  # –û—Ç 5 –¥–æ 2 —Å–ª–æ–≤
            if len(words) >= word_count * 2:
                first_block = " ".join(words[:word_count]).lower()
                # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                first_block_clean = re.sub(r"[^\w\s]", "", first_block)
                next_block_clean = re.sub(
                    r"[^\w\s]", "", " ".join(words[word_count : word_count * 2]).lower()
                )

                if first_block_clean == next_block_clean:
                    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç –±–ª–æ–∫–∞
                    text = " ".join(words[:word_count] + words[word_count * 2 :])
                    words = text.split()
                    break

        # –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ–≤–∞ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ö
        # –ù–∞–ø—Ä–∏–º–µ—Ä: "–ñ–∏–≤—É" ‚Üí "–ñ–∏–≤—É, –∂–∏–≤—É" –∏–ª–∏ "–∂–∏–≤—É –ñ–∏–≤—É"
        if len(words) >= 2:
            first_word_lower = words[0].lower().strip()
            second_word_lower = words[1].lower().strip()
            # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
            first_clean = re.sub(r"[^\w]", "", first_word_lower)
            second_clean = re.sub(r"[^\w]", "", second_word_lower)

            if first_clean == second_clean and first_clean:
                # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç
                text = " ".join([words[0]] + words[2:])
                words = text.split()

    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ 15 –¥–ª—è –ª–æ–≤–ª–∏ –ø–æ–≤—Ç–æ—Ä–æ–≤ —Ç–∏–ø–∞ ¬´–ü—Ä–∏–≤–µ—Ç! To be ‚Äî —ç—Ç–æ –≥–ª–∞–≥–æ–ª...¬ª)
    text = remove_duplicate_text(text, min_length=15)

    # –£–¥–∞–ª—è–µ–º –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏–µ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∞–±–∑–∞—Ü—ã (—Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏)
    paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]
    if len(paragraphs) >= 2:
        unique_ordered = [paragraphs[0]]
        for i in range(1, len(paragraphs)):
            prev_norm = re.sub(r"\s+", " ", unique_ordered[-1].lower().strip())
            curr_norm = re.sub(r"\s+", " ", paragraphs[i].lower().strip())
            if curr_norm != prev_norm or len(curr_norm) < 20:
                unique_ordered.append(paragraphs[i])
        text = "\n\n".join(unique_ordered)

    # –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –º–æ–¥–µ–ª–∏: "2dot 6" ‚Üí "2¬∑6", "—Ö } = -2" ‚Üí "—Ö = -2"
    text = re.sub(r"(\d+)dot\s+(\d+)", r"\1¬∑\2", text, flags=re.IGNORECASE)
    text = re.sub(r"(\w)\s*\}\s*=\s*", r"\1 = ", text)

    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —É–º–Ω–æ–∂–µ–Ω–∏—è
    # –ü–∞—Ç—Ç–µ—Ä–Ω 1: "1. 3 1 = 3" ‚Üí "1. 3 √ó 1 = 3" (–Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ - —Å–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Ö)
    text = re.sub(r"(\d+\.\s+)(\d+)\s+(\d+)\s*=\s*(\d+)", r"\1\2 √ó \3 = \4", text)
    # –ü–∞—Ç—Ç–µ—Ä–Ω 2: "3 1 = 3" ‚Üí "3 √ó 1 = 3" (–æ–±—ã—á–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è, –Ω–æ –Ω–µ –µ—Å–ª–∏ –ø–µ—Ä–µ–¥ –ø–µ—Ä–≤—ã–º —á–∏—Å–ª–æ–º –µ—Å—Ç—å —Ç–æ—á–∫–∞)
    text = re.sub(r"(?<!\d\.\s)(?<!\d\.)(\d+)\s+(\d+)\s*=\s*(\d+)", r"\1 √ó \2 = \3", text)
    # –ü–∞—Ç—Ç–µ—Ä–Ω 3: "3*3=9" ‚Üí "3 √ó 3 = 9"
    text = re.sub(r"(\d+)\*(\d+)\s*=\s*(\d+)", r"\1 √ó \2 = \3", text)

    # –§–∏–∑–∏–∫–∞: Delta t / Delta T ‚Üí Œît / ŒîT; –≤ —Ñ–æ—Ä–º—É–ª–∞—Ö –±—É–∫–≤–∞ x –∫–∞–∫ —É–º–Ω–æ–∂–µ–Ω–∏–µ ‚Üí ¬∑
    text = re.sub(r"\bDelta\s+t\b", "Œît", text, flags=re.IGNORECASE)
    text = re.sub(r"\bDelta\s+T\b", "ŒîT", text)
    # –û–ø–µ—Ä–∞–Ω–¥ x –æ–ø–µ—Ä–∞–Ω–¥ (—á–∏—Å–ª–æ –∏–ª–∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–∏–ø–∞ log_a, b, c) ‚Üí ¬∑
    text = re.sub(
        r"(\b\d+\b|\b[a-zA-Z_][a-zA-Z0-9_]*\b)\s+x\s+(\b\d+\b|\b[a-zA-Z_][a-zA-Z0-9_]*\b)",
        r"\1 ¬∑ \2",
        text,
    )

    # –°—Ç–µ–ø–µ–Ω—å –≤ —Ñ–æ—Ä–º—É–ª–∞—Ö: ^2 ^3 ^4 ^5 ‚Üí ¬≤ ¬≥ ‚Å¥ ‚Åµ (—Ç–∏–ø–æ–≥—Ä–∞—Ñ—Å–∫–∏–π —Å—Ç–∏–ª—å)
    text = re.sub(r"(\w)\^2\b", r"\1¬≤", text)
    text = re.sub(r"(\w)\^3\b", r"\1¬≥", text)
    text = re.sub(r"(\w)\^4\b", r"\1‚Å¥", text)
    text = re.sub(r"(\w)\^5\b", r"\1‚Åµ", text)

    # –û–±—Ä–µ–∑–∫–∞ –º—É—Å–æ—Ä–∞ –≤ –∫–æ–Ω—Ü–µ: —Å–∫–ª–µ–µ–Ω–Ω—ã–µ ¬´Q_–î–ª—è —Ä–µ—à–µ–Ω–∏—è‚Ä¶¬ª, ¬´t_–î–ª—è‚Ä¶¬ª ‚Äî —É–±–∏—Ä–∞–µ–º –¥–æ –∫–æ–Ω—Ü–∞ —Å—Ç—Ä–æ–∫–∏
    text = re.sub(r"\s+[A-Za-z]_[–ê-–Ø–∞-—è—ë]\S*(?:\s+\S+)*\s*$", "", text)

    # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫ –¥–æ–ª–ª–∞—Ä–∞ (–æ–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª–∏ —Ñ–æ—Ä–º—É–ª –≤ Telegram/Markdown)
    text = text.replace("$", "")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º ** (markdown –∂–∏—Ä–Ω—ã–π) ‚Äî —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥ —Ä–µ–Ω–¥–µ—Ä–∏—Ç –∂–∏—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ª—É—á—à–µ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è

    # –ó–∞–º–µ–Ω—è–µ–º LaTeX –∫–æ–º–∞–Ω–¥—ã –Ω–∞ —Ç–∏–ø–æ–≥—Ä–∞—Ñ—Å–∫–∏–π —Å—Ç–∏–ª—å (–∫–∞–∫ –≤ –ø—Ä–æ–º–ø—Ç–µ: ‚àö, –Ω–µ sqrt())
    text = re.sub(r"\\sqrt\s*\{([^}]*)\}", r"‚àö(\1)", text)
    # \vec{v} ‚Üí v (–≤–µ–∫—Ç–æ—Ä–Ω–∞—è –≤–µ–ª–∏—á–∏–Ω–∞ –≤ —Ñ–æ—Ä–º—É–ª–∞—Ö)
    text = re.sub(r"\\vec\s*\{([^}]*)\}", r"\1", text, flags=re.IGNORECASE)
    # \nabla ‚Üí ‚àá, \partial ‚Üí ‚àÇ (—Å–∏–º–≤–æ–ª—ã –¥–ª—è —Ñ–∏–∑–∏–∫–∏/–º–∞—Ç–µ–º–∞—Ç–∏–∫–∏)
    text = re.sub(r"\\nabla\b", "‚àá", text, flags=re.IGNORECASE)
    text = re.sub(r"\\partial\b", "‚àÇ", text, flags=re.IGNORECASE)
    # –û—Å—Ç–∞–ª—å–Ω—ã–µ LaTeX –∫–æ–º–∞–Ω–¥—ã
    latex_to_text = {
        r"\\log": "log",
        r"\\ln": "ln",
        r"\\sin": "sin",
        r"\\cos": "cos",
        r"\\tan": "tan",
        r"\\cot": "cot",
        r"\\sqrt": "‚àö",
        r"\\sum": "sum",
        r"\\int": "integral",
        r"\\lim": "lim",
        r"\\infty": "infinity",
        r"\\alpha": "alpha",
        r"\\beta": "beta",
        r"\\gamma": "gamma",
        r"\\pi": "pi",
        r"\\times": "√ó",
        r"\\cdot": "√ó",
        r"\\div": "√∑",
        r"\\pm": "+-",
        r"\\leq": "<=",
        r"\\geq": ">=",
        r"\\neq": "!=",
        r"\\approx": "~",
    }
    for latex, replacement in latex_to_text.items():
        text = re.sub(latex, replacement, text, flags=re.IGNORECASE)

    # –£–±–∏—Ä–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è LaTeX –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ —Å–∫–æ–±–∫–∏
    latex_patterns = [
        r"\\begin\{[^}]+\}.*?\\end\{[^}]+\}",  # –û–∫—Ä—É–∂–µ–Ω–∏—è
        r"\\frac\{([^}]+)\}\{([^}]+)\}",  # \frac{a}{b} ‚Üí (a)/(b)
        r"\\\[",  # \[
        r"\\\]",  # \]
        r"\\\{",  # \{
        r"\\\}",  # \}
        r"\\\(",  # \(
        r"\\\)",  # \)
    ]
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥—Ä–æ–±–µ–π
    text = re.sub(r"\\frac\{([^}]+)\}\{([^}]+)\}", r"(\1)/(\2)", text)
    # –£–¥–∞–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è LaTeX –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
    for pattern in latex_patterns:
        text = re.sub(pattern, "", text, flags=re.IGNORECASE | re.DOTALL)
    # –£–¥–∞–ª—è–µ–º –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ LaTeX –∫–æ–º–∞–Ω–¥—ã (–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ –Ω–∏—Ö)
    text = re.sub(r"\\([a-zA-Z]+)(?=_)", r"\1", text)  # \log_a ‚Üí log_a
    text = re.sub(r"\\([a-zA-Z]+)", r"\1", text)  # \unknown ‚Üí unknown

    # –£–¥–∞–ª—è–µ–º —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –ø—Ä–æ ¬´–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é/–≥—Ä–∞—Ñ–∏–∫/—Ç–∞–±–ª–∏—Ü—É, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –ø–æ–∫–∞–∑–∞–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏¬ª.
    # –í–∞–∂–Ω–æ: –¥–µ–ª–∞–µ–º —ç—Ç–æ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ, —á—Ç–æ–±—ã —Ç–∞–∫–∏–µ —Ñ—Ä–∞–∑—ã –Ω–µ –ø—Ä–æ—Å–∫–∞–∫–∏–≤–∞–ª–∏
    # –Ω–∏ –≤ –æ–¥–Ω–æ–º –æ—Ç–≤–µ—Ç–µ (–¥–ª—è –ª—é–±—ã—Ö –ø—Ä–µ–¥–º–µ—Ç–æ–≤ –∏ —Ç–∏–ø–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤).
    auto_system_patterns = [
        # ¬´–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –ø–æ–∫–∞–∑–∞–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏¬ª
        r"[–í–≤]–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏[—è–∏—é]\s+.{0,30}?\s*(?:–±—É–¥–µ—Ç|–ø–æ—è–≤–∏—Ç—Å—è|–ø–æ–∫–∞–∂–µ—Ç—Å—è)\s+–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫[–∞-—è—ë]*\.?",
        r"[–í–≤]–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏[—è–∏—é]\s+.{0,30}?\s*—Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è\s+–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫[–∞-—è—ë]*\.?",
        # ¬´–≥—Ä–∞—Ñ–∏–∫ –±—É–¥–µ—Ç –ø–æ–∫–∞–∑–∞–Ω/–ø–æ—è–≤–∏—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏¬ª
        r"[–ì–≥]—Ä–∞—Ñ–∏–∫\s+.{0,30}?\s*(?:–±—É–¥–µ—Ç\s+–ø–æ–∫–∞–∑–∞–Ω|–ø–æ—è–≤–∏—Ç—Å—è|—Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è)\s+–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫[–∞-—è—ë]*\.?",
        # ¬´—Ç–∞–±–ª–∏—Ü–∞ –±—É–¥–µ—Ç –ø–æ–∫–∞–∑–∞–Ω–∞/—Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è¬ª
        r"[–¢—Ç]–∞–±–ª–∏—Ü[–∞–µ—ã]\s+.{0,30}?\s*(?:–±—É–¥–µ—Ç\s+–ø–æ–∫–∞–∑–∞–Ω[–∞—ã]?|–ø–æ—è–≤–∏—Ç—Å—è|—Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è)\s+–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫[–∞-—è—ë]*\.?",
        # ¬´–¥–∏–∞–≥—Ä–∞–º–º–∞/—Å—Ö–µ–º–∞/–∫–∞—Ä—Ç–∞ –±—É–¥–µ—Ç –ø–æ–∫–∞–∑–∞–Ω–∞¬ª
        r"(?:[–î–¥]–∏–∞–≥—Ä–∞–º–º[–∞—ã]|[–°—Å]—Ö–µ–º[–∞—ã]|[–ö–∫]–∞—Ä—Ç[–∞—ã])\s+.{0,30}?\s*(?:–±—É–¥–µ—Ç\s+–ø–æ–∫–∞–∑–∞–Ω[–∞—ã]?|–ø–æ—è–≤–∏—Ç—Å—è|—Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è)\s+–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫[–∞-—è—ë]*\.?",
        # ¬´—Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç/–Ω–∞—Ä–∏—Å—É–µ—Ç/–ø–æ–∫–∞–∂–µ—Ç ...¬ª
        r"(?:—Å–∏—Å—Ç–µ–º[–∞–µ—ã]?\s+)?–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫[–∞-—è—ë]+\s+—Å–≥–µ–Ω–µ—Ä–∏—Ä—É[–µ—é]—Ç?\s+[^.!?\n]+",
        r"(?:—Å–∏—Å—Ç–µ–º[–∞–µ—ã]?\s+)?–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫[–∞-—è—ë]+\s+–Ω–∞—Ä–∏—Å—É[–µ—é]—Ç?\s+[^.!?\n]+",
        r"(?:—Å–∏—Å—Ç–µ–º[–∞–µ—ã]?\s+)?–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫[–∞-—è—ë]+\s+–ø–æ–∫–∞–∂[–µtu][—Ç–º]?\s+[^.!?\n]+",
        # ¬´—Å–∏—Å—Ç–µ–º–∞ —É–∂–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞ / —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–∏—Ç ...¬ª
        r"—Å–∏—Å—Ç–µ–º[–∞–µ—ã]?\s+—É–∂–µ\s+—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª[–∞–∏]?\s+[^.!?\n]+",
        r"—Å–∏—Å—Ç–µ–º[–∞–µ—ã]?\s+—Å–≥–µ–Ω–µ—Ä–∏—Ä—É[–µ—é]—Ç?\s+[^.!?\n]+–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫[–∞-—è—ë]+\s*[^.!?\n]*",
        r"—Å–∏—Å—Ç–µ–º[–∞–µ—ã]?\s+–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫[–∞-—è—ë]+\s+–¥–æ–±–∞–≤–∏—Ç\s+[^.!?\n]+",
        # –û–±—â–µ–µ: ¬´—Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ ...¬ª –±–µ–∑ —É—Ç–æ—á–Ω–µ–Ω–∏—è, —á—Ç–æ –∏–º–µ–Ω–Ω–æ
        r"—Å–∏—Å—Ç–µ–º[–∞–µ—ã]?\s+–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫[–∞-—è—ë]+[^.!?\n]*",
        # –ê–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
        r"system\s+will\s+automatically[^.!?\n]*",
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —á–∞—Å—Ç—ã—Ö —Å–ª—É—á–∞–µ–≤
        r"–±—É–¥–µ—Ç\s+–ø–æ–∫–∞–∑–∞–Ω[–∞–æ—ã]?\s+–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏\.?",
        r"–ø–æ—è–≤–∏—Ç—Å—è\s+–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏\.?",
        r"—Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è\s+–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏\.?",
    ]
    for pattern in auto_system_patterns:
        text = re.sub(pattern, "", text, flags=re.IGNORECASE)

    # –£–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–∏–º–≤–æ–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –ª–æ–º–∞—é—Ç –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–ª–∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è
    # –ù–ï —É–¥–∞–ª—è–µ–º: √ó ‚Ä¢ (–±—É–ª–ª–µ—Ç—ã, —É–º–Ω–æ–∂–µ–Ω–∏–µ); ¬≤ ¬≥ ‚àë ‚à´ ‚àû ‚à† ¬∞ (—Ñ–æ—Ä–º—É–ª—ã ‚Äî —Å–º. prompts.py –ó–ê–ü–ò–°–¨ –§–û–†–ú–£–õ)
    complex_math_chars = []
    for char in complex_math_chars:
        text = text.replace(char, "")

    # –û—á–∏—â–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã (–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∞–±–∑–∞—Ü—ã - –¥–≤–æ–π–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫)
    text = re.sub(r"[ \t]+", " ", text)  # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
    text = re.sub(r"\n\s*\n\s*\n+", "\n\n", text)  # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –≤ –¥–≤–∞
    text = text.strip()

    return text


class IModerator(ABC):
    """
    –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –º–æ–¥–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.

    –°–ª–µ–¥—É–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø—É Interface Segregation (ISP).
    """

    @abstractmethod
    def moderate(self, text: str) -> tuple[bool, str]:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–∞–≤–∏–ª–∞–º –º–æ–¥–µ—Ä–∞—Ü–∏–∏.

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏.

        Returns:
            tuple[bool, str]: (is_safe, reason)
        """
        pass


class IContextBuilder(ABC):
    """
    –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è AI.

    –°–ª–µ–¥—É–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø—É Interface Segregation (ISP).
    """

    @abstractmethod
    def build(
        self, user_message: str, chat_history: list[dict] = None, user_age: int | None = None
    ) -> str:
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ AI.

        Args:
            user_message: –¢–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
            chat_history: –ò—Å—Ç–æ—Ä–∏—è –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π.
            user_age: –í–æ–∑—Ä–∞—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.

        Returns:
            str: –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è AI –º–æ–¥–µ–ª–∏.
        """
        pass


class YandexAIResponseGenerator:
    """
    –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç–≤–µ—Ç–æ–≤ AI —á–µ—Ä–µ–∑ Yandex Cloud (YandexGPT).

    –ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ AI.
    –ú–æ–¥–µ—Ä–∞—Ü–∏—è –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–µ–ª–µ–≥–∏—Ä—É—é—Ç—Å—è —á–µ—Ä–µ–∑ Dependency Injection (SOLID).
    """

    def __init__(
        self,
        moderator: IModerator,
        context_builder: IContextBuilder,
        knowledge_service=None,  # type: ignore
        yandex_service=None,  # type: ignore
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –æ—Ç–≤–µ—Ç–æ–≤.

        Args:
            moderator: –°–µ—Ä–≤–∏—Å –º–æ–¥–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.
            context_builder: –°–µ—Ä–≤–∏—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
            knowledge_service: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ - —Å–µ—Ä–≤–∏—Å –∑–Ω–∞–Ω–∏–π (–¥–ª—è DI).
                –ï—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–∏–Ω–≥–ª—Ç–æ–Ω.
            yandex_service: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ - Yandex Cloud —Å–µ—Ä–≤–∏—Å (–¥–ª—è DI).
                –ï—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–∏–Ω–≥–ª—Ç–æ–Ω.
        """
        self.moderator = moderator
        self.context_builder = context_builder

        # Dependency Injection: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã –∏–ª–∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–∏–Ω–≥–ª—Ç–æ–Ω—ã
        # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Å –º–æ–∫–∞–º–∏ –∏ —É–ª—É—á—à–∞–µ—Ç —Å–æ–±–ª—é–¥–µ–Ω–∏–µ DIP
        self.knowledge_service = (
            knowledge_service if knowledge_service is not None else get_knowledge_service()
        )
        self.yandex_service = (
            yandex_service if yandex_service is not None else get_yandex_cloud_service()
        )

        logger.info("‚úÖ Yandex AI Response Generator –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def _should_use_wikipedia(self, user_message: str) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –Ω—É–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç—Ç–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.

        –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: Wikipedia –¥–æ–ª–∂–Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –í–°–ï–• –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤!
        –ò—Å–∫–ª—é—á–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è —á–∏—Å—Ç–æ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á.

        Args:
            user_message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

        Returns:
            bool: True –µ—Å–ª–∏ —Å—Ç–æ–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
        """
        message_lower = user_message.lower().strip()

        # –ò—Å–∫–ª—é—á–∞–µ–º –¢–û–õ–¨–ö–û —á–∏—Å—Ç–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ/–≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        # –î–ª—è –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º Wikipedia!
        exclude_patterns = [
            r"^\d+\s*[\+\-\*\/√ó√∑]\s*\d+",  # –ß–∏—Å—Ç—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è: "5 + 3", "7 √ó 8"
            r"^—Å–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç\s+\d+",  # "–°–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 5+3"
            r"^—Ä–µ—à–∏\s+\d+",  # "–†–µ—à–∏ 5+3"
            r"^–ø–æ—Å—á–∏—Ç–∞–π\s+\d+",  # "–ü–æ—Å—á–∏—Ç–∞–π 5+3"
            r"^–≤—ã—á–∏—Å–ª–∏\s+\d+",  # "–í—ã—á–∏—Å–ª–∏ 5+3"
            r"–ø–æ–∫–∞–∂–∏\s+—Ç–∞–±–ª–∏—Ü—É\s+—É–º–Ω–æ–∂–µ–Ω–∏—è",  # –¢–∞–±–ª–∏—Ü–∞ —É–º–Ω–æ–∂–µ–Ω–∏—è - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
            r"–Ω–∞—Ä–∏—Å—É–π\s+–≥—Ä–∞—Ñ–∏–∫",  # –ì—Ä–∞—Ñ–∏–∫–∏ - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
            r"–ø–æ—Å—Ç—Ä–æ–π\s+–≥—Ä–∞—Ñ–∏–∫",  # –ì—Ä–∞—Ñ–∏–∫–∏
            r"–ø–æ–∫–∞–∂–∏\s+–≥—Ä–∞—Ñ–∏–∫",  # –ì—Ä–∞—Ñ–∏–∫–∏
            r"–ø—Ä–∏–≤–µ—Ç",  # –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è
            r"^–∫–∞–∫\s+(?:—Ç–µ–±—è|—Ç–≤–æ—è)\s+(?:–∑–æ–≤—É—Ç|–∏–º—è)",  # –í–æ–ø—Ä–æ—Å—ã –æ –±–æ—Ç–µ
        ]

        # –î–ª—è –í–°–ï–• –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º Wikipedia (–∫—Ä–æ–º–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–π)
        # - "—á—Ç–æ —Ç–∞–∫–æ–µ —Ñ–æ—Ç–æ—Å–∏–Ω—Ç–µ–∑" - –¥–∞
        # - "–∫—Ç–æ —Ç–∞–∫–æ–π –ü—É—à–∫–∏–Ω" - –¥–∞
        # - "–ø–æ—á–µ–º—É –Ω–µ–±–æ –≥–æ–ª—É–±–æ–µ" - –¥–∞
        # - "–∫–∞–∫–∞—è —Å—Ç–æ–ª–∏—Ü–∞ –§—Ä–∞–Ω—Ü–∏–∏" - –¥–∞
        # - "—Ä–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ –í–û–í" - –¥–∞
        # - "–≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ö–∏—Ç–∞–π" - –¥–∞
        # - "–≤ –∫–∞–∫–æ–º –≥–æ–¥—É –±—ã–ª–∞ –≤–æ–π–Ω–∞" - –¥–∞
        # - –ª—é–±—ã–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã - –¥–∞!
        return all(not re.search(pattern, message_lower) for pattern in exclude_patterns)

    async def generate_response(
        self,
        user_message: str,
        chat_history: list[dict] = None,
        user_age: int | None = None,
        user_name: str | None = None,
        user_grade: int | None = None,
        is_history_cleared: bool = False,
        message_count_since_name: int = 0,
        skip_name_asking: bool = False,  # noqa: ARG002
        non_educational_questions_count: int = 0,
        is_premium: bool = False,  # noqa: ARG002
        is_auto_greeting_sent: bool = False,
        user_gender: str | None = None,
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç AI –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Pro –º–æ–¥–µ–ª—å –¥–ª—è –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.
        –õ–∏–º–∏—Ç—ã –∑–∞–ø—Ä–æ—Å–æ–≤ —É–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —á–µ—Ä–µ–∑ premium_features_service.

        Args:
            user_message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
            chat_history: –ò—Å—Ç–æ—Ä–∏—è –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π.
            user_age: –í–æ–∑—Ä–∞—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏.
            user_name: –ò–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –æ–±—Ä–∞—â–µ–Ω–∏—è.
            is_history_cleared: –§–ª–∞–≥ –æ—á–∏—Å—Ç–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏.
            message_count_since_name: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±—Ä–∞—â–µ–Ω–∏—è –ø–æ –∏–º–µ–Ω–∏.
            skip_name_asking: –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∏–º–µ–Ω–∏ (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —Ç–µ–∫—É—â–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏).
            non_educational_questions_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–ø—Ä–µ–¥–º–µ—Ç–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ–¥—Ä—è–¥.
            is_premium: Premium —Å—Ç–∞—Ç—É—Å (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –æ—Å—Ç–∞–≤–ª–µ–Ω–æ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
            is_auto_greeting_sent: –§–ª–∞–≥, —á—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ —É–∂–µ –±—ã–ª–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ

        Returns:
            str: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç AI.
        """
        try:
            # –ü—Ä–∞–≤–∏–ª–∞ –ø–æ –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–º —Ç–µ–º–∞–º –æ—Ç–∫–ª—é—á–µ–Ω—ã ‚Äî –º–æ–¥–µ—Ä–∞—Ü–∏—è –Ω–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è

            # RAG: enhanced_search –ø–æ–¥—Ç—è–≥–∏–≤–∞–µ—Ç Wikipedia –ø—Ä–∏ –ø—É—Å—Ç–æ–π –±–∞–∑–µ (use_wikipedia)
            relevant_materials = await self.knowledge_service.enhanced_search(
                user_question=user_message,
                user_age=user_age,
                top_k=3,
                use_wikipedia=self._should_use_wikipedia(user_message),
            )
            web_context = self.knowledge_service.format_knowledge_for_ai(relevant_materials)

            if web_context:
                compressor = ContextCompressor()
                web_context = compressor.compress(
                    context=web_context,
                    question=user_message,
                    max_sentences=7,
                )

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –≤ —Ñ–æ—Ä–º–∞—Ç Yandex Cloud
            yandex_history = []
            if chat_history:
                for msg in chat_history[-10:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å–æ–æ–±—â–µ–Ω–∏–π
                    role = msg.get("role", "user")  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–æ–ª—å –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
                    text = msg.get("text", "").strip()
                    if text:  # –¢–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
                        yandex_history.append({"role": role, "text": text})

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º PromptBuilder –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞
            prompt_builder = get_prompt_builder()
            # –ï—Å–ª–∏ is_auto_greeting_sent –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
            if not is_auto_greeting_sent and chat_history:
                for msg in chat_history:
                    if msg.get("role") == "assistant":
                        msg_text = msg.get("text", "").lower()
                        if (
                            "–ø—Ä–∏–≤–µ—Ç" in msg_text
                            or "–Ω–∞—á–Ω–µ–º" in msg_text
                            or "—á–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å" in msg_text
                        ):
                            is_auto_greeting_sent = True
                            break

            enhanced_system_prompt = prompt_builder.build_system_prompt(
                user_message=user_message,
                user_name=user_name,
                chat_history=chat_history,
                is_history_cleared=is_history_cleared,
                message_count_since_name=message_count_since_name,
                non_educational_questions_count=non_educational_questions_count,
                user_age=user_age,
                user_grade=user_grade,
                is_auto_greeting_sent=is_auto_greeting_sent,
                user_gender=user_gender,
            )

            if web_context:
                enhanced_system_prompt += f"\n\nüìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n{web_context}\n\n"

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Pro –º–æ–¥–µ–ª—å –¥–ª—è –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π (YandexGPT Pro Latest - —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è)
            # –§–æ—Ä–º–∞—Ç yandexgpt-pro/latest - Pro –≤–µ—Ä—Å–∏—è YandexGPT
            model_name = settings.yandex_gpt_model
            temperature = settings.ai_temperature  # –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
            max_tokens = settings.ai_max_tokens  # –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ Yandex Cloud
            logger.info("üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ YandexGPT Pro...")
            response = await self.yandex_service.generate_text_response(
                user_message=user_message,  # –ü–µ—Ä–µ–¥–∞–µ–º —á–∏—Å—Ç–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                chat_history=yandex_history,
                system_prompt=enhanced_system_prompt,
                temperature=temperature,
                max_tokens=max_tokens,
                model=model_name,
            )

            if response:
                # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ LaTeX
                cleaned_response = clean_ai_response(response.strip())
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –≤–æ–ø—Ä–æ—Å –¥–ª—è –≤–æ–≤–ª–µ—á–µ–Ω–∏—è
                final_response = add_random_engagement_question(cleaned_response)
                return final_response
            else:
                return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —Å–º–æ–≥ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ AI (Yandex): {e}")
            return "–û–π, —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."

    def get_model_info(self) -> dict[str, str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏ AI.

        Returns:
            Dict[str, str]: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏ Yandex Cloud.
        """
        return {
            "provider": "Yandex Cloud",
            "model": settings.yandex_gpt_model,
            "temperature": str(settings.ai_temperature),
            "max_tokens": str(settings.ai_max_tokens),
            "public_name": "PandaPalAI (powered by YandexGPT)",
        }

    async def analyze_image(
        self,
        image_data: bytes,
        user_message: str | None = None,
        user_age: int | None = None,  # noqa: ARG002
    ) -> str:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Yandex Vision + YandexGPT.

        Args:
            image_data: –î–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –±–∞–π—Ç–∞—Ö.
            user_message: –°–æ–ø—Ä–æ–≤–æ–∂–¥–∞—é—â–∏–π —Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
            user_age: –í–æ–∑—Ä–∞—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏.

        Returns:
            str: –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        """
        try:
            logger.info("üì∑ –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ Yandex Vision + GPT...")

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Yandex Vision + GPT
            analysis_result = await self.yandex_service.analyze_image_with_text(
                image_data=image_data, user_question=user_message
            )

            if not analysis_result.get("has_text") and not analysis_result.get("analysis"):
                return (
                    "üì∑ –Ø –Ω–µ —Å–º–æ–≥ —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏.\n\n"
                    "–ü–æ–ø—Ä–æ–±—É–π —Å—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞–Ω–∏–µ –±–æ–ª–µ–µ —á–µ—Ç–∫–æ! üìù"
                )

            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
            response_parts = []

            if analysis_result.get("recognized_text"):
                response_parts.append(
                    f"üìù <b>–¢–µ–∫—Å—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏:</b>\n{analysis_result['recognized_text']}\n"
                )

            if analysis_result.get("analysis"):
                # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º
                cleaned_analysis = clean_ai_response(analysis_result["analysis"])
                response_parts.append(f"üéì <b>–†–∞–∑–±–æ—Ä –∑–∞–¥–∞–Ω–∏—è:</b>\n{cleaned_analysis}")

            result = "\n".join(response_parts)
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –≤—Å–µ–≥–æ –æ—Ç–≤–µ—Ç–∞
            cleaned_result = clean_ai_response(result)
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –≤–æ–ø—Ä–æ—Å –¥–ª—è –≤–æ–≤–ª–µ—á–µ–Ω–∏—è
            return add_random_engagement_question(cleaned_result)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (Yandex): {e}")
            return "üòî –ò–∑–≤–∏–Ω–∏, —É –º–µ–Ω—è –≤–æ–∑–Ω–∏–∫–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã —Å –∞–Ω–∞–ª–∏–∑–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑!"

    async def moderate_image_content(self, image_data: bytes) -> tuple[bool, str]:  # noqa: ARG002
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å.

        Args:
            image_data: –î–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –±–∞–π—Ç–∞—Ö.

        Returns:
            tuple[bool, str]: (is_safe, reason)
        """
        # –ü—Ä–∞–≤–∏–ª–∞ –ø–æ –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–º —Ç–µ–º–∞–º –æ—Ç–∫–ª—é—á–µ–Ω—ã ‚Äî –Ω–µ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è
        return True, "OK"
