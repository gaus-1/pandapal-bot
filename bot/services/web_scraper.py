"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —É—á–µ–±–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤.

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Ä–µ–∞–ª–∏–∑—É–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–±–æ—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
—Å –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∞–π—Ç–æ–≤, –≤–∫–ª—é—á–∞—è –∑–∞–¥–∞—á–∏, –Ω–æ–≤–æ—Å—Ç–∏ –∏ —É—á–µ–±–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã.

–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å (OWASP A10 - SSRF):
- –í—Å–µ –≤–Ω–µ—à–Ω–∏–µ URL –≤–∞–ª–∏–¥–∏—Ä—É—é—Ç—Å—è —á–µ—Ä–µ–∑ SSRFProtection
- –†–∞–∑—Ä–µ—à–µ–Ω—ã —Ç–æ–ª—å–∫–æ whitelisted –¥–æ–º–µ–Ω—ã
- –ë–ª–æ–∫–∏—Ä—É—é—Ç—Å—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ IP –∏ localhost
"""

import asyncio
import re
from dataclasses import dataclass
from datetime import datetime
from urllib.parse import urljoin

import aiohttp
from bs4 import BeautifulSoup
from loguru import logger

from bot.security import SSRFProtection, validate_url_safety


@dataclass
class EducationalContent:
    """
    –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.

    Attributes:
        title (str): –ù–∞–∑–≤–∞–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞ –∏–ª–∏ –∑–∞–¥–∞—á–∏.
        content (str): –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞.
        subject (str): –£—á–µ–±–Ω—ã–π –ø—Ä–µ–¥–º–µ—Ç (–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞, —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –∏ —Ç.–¥.).
        difficulty (str): –£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (–ª–µ–≥–∫–∏–π, —Å—Ä–µ–¥–Ω–∏–π, —Å–ª–æ–∂–Ω—ã–π).
        source_url (str): URL –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–∞.
        extracted_at (datetime): –í—Ä–µ–º—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞.
        tags (List[str]): –¢–µ–≥–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –ø–æ–∏—Å–∫–∞.
    """

    title: str
    content: str
    subject: str
    difficulty: str
    source_url: str
    extracted_at: datetime
    tags: list[str]


@dataclass
class NewsItem:
    """
    –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π.

    Attributes:
        title (str): –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏.
        content (str): –¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏.
        url (str): –°—Å—ã–ª–∫–∞ –Ω–∞ –ø–æ–ª–Ω—É—é –≤–µ—Ä—Å–∏—é.
        published_date (Optional[datetime]): –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞).
        source (str): –ò—Å—Ç–æ—á–Ω–∏–∫ –Ω–æ–≤–æ—Å—Ç–∏ (nsportal.ru, school203.spb.ru –∏ —Ç.–¥.).
    """

    title: str
    content: str
    url: str
    published_date: datetime | None
    source: str


class WebScraperService:
    """
    –°–µ—Ä–≤–∏—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∞–π—Ç–æ–≤.

    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–¥–∞—á, –Ω–æ–≤–æ—Å—Ç–µ–π –∏ —É—á–µ–±–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
    —Å nsportal.ru –∏ school203.spb.ru.
    """

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞."""
        self.session: aiohttp.ClientSession | None = None
        self.user_agent = (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å–∞–π—Ç–æ–≤
        self.nsportal_config = {
            "base_url": "https://nsportal.ru",
            "library_url": "https://nsportal.ru/shkola",
            "news_url": "https://nsportal.ru/novosti",
            "subjects": [
                "matematika",
                "russkiy-yazyk",
                "literatura",
                "istoriya",
                "geografiya",
                "biologiya",
                "fizika",
                "khimiya",
                "informatika",
            ],
        }

        self.school203_config = {
            "base_url": "https://school203.spb.ru",
            "sections": [
                "uchebnaya-deyatelnost",
                "vospitatelnaya-rabota",
                "dopolnitelnoe-obrazovanie",
                "metodicheskaya-rabota",
            ],
        }

        logger.info("üåê WebScraperService –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—Ö–æ–¥."""
        await self.start_session()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):  # noqa: ARG002
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—ã—Ö–æ–¥."""
        await self.close_session()

    async def start_session(self):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å HTTP —Å–µ—Å—Å–∏—é."""
        if not self.session:
            timeout = aiohttp.ClientTimeout(total=30)
            self.session = aiohttp.ClientSession(
                timeout=timeout, headers={"User-Agent": self.user_agent}
            )

    async def close_session(self):
        """–ó–∞–∫—Ä—ã—Ç—å HTTP —Å–µ—Å—Å–∏—é."""
        if self.session:
            await self.session.close()
            self.session = None

    async def _safe_get(self, url: str) -> aiohttp.ClientResponse | None:
        """
        –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π HTTP GET –∑–∞–ø—Ä–æ—Å —Å SSRF –∑–∞—â–∏—Ç–æ–π (OWASP A10).

        Args:
            url: URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞

        Returns:
            Response –æ–±—ä–µ–∫—Ç –∏–ª–∏ None –µ—Å–ª–∏ URL –Ω–µ–±–µ–∑–æ–ø–∞—Å–µ–Ω
        """
        # –í–∞–ª–∏–¥–∞—Ü–∏—è URL –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º
        if not validate_url_safety(url):
            logger.warning(f"üö´ SSRF: –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω—ã–π URL: {url}")
            return None

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ SSRFProtection
        if not SSRFProtection.validate_external_request(url, "GET"):
            logger.warning(f"üö´ SSRF: –∑–∞–ø—Ä–æ—Å –∫ {url} –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –ø–æ–ª–∏—Ç–∏–∫–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏")
            return None

        try:
            return await self.session.get(url)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ HTTP –∑–∞–ø—Ä–æ—Å–∞ –∫ {url}: {e}")
            return None

    async def scrape_nsportal_tasks(self, limit: int = 50) -> list[EducationalContent]:
        """
        –ü–∞—Ä—Å–∏—Ç—å –∑–∞–¥–∞—á–∏ —Å nsportal.ru.

        Args:
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è.

        Returns:
            List[EducationalContent]: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á.
        """
        tasks = []

        try:
            # –ü–∞—Ä—Å–∏–º –±–∏–±–ª–∏–æ—Ç–µ–∫—É –ø–æ –ø—Ä–µ–¥–º–µ—Ç–∞–º
            for subject in self.nsportal_config["subjects"]:
                subject_url = f"{self.nsportal_config['library_url']}/{subject}"
                subject_tasks = await self._scrape_nsportal_subject(
                    subject_url, subject, limit // len(self.nsportal_config["subjects"])
                )
                tasks.extend(subject_tasks)

                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                await asyncio.sleep(1)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ nsportal.ru: {e}")

        return tasks[:limit]

    async def _scrape_nsportal_subject(
        self, url: str, subject: str, limit: int
    ) -> list[EducationalContent]:
        """–ü–∞—Ä—Å–∏—Ç—å –∑–∞–¥–∞—á–∏ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –ø—Ä–µ–¥–º–µ—Ç—É —Å nsportal.ru."""
        tasks = []

        try:
            response = await self._safe_get(url)
            if response and response.status == 200:
                html = await response.text()
                soup = BeautifulSoup(html, "html.parser")

                # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
                material_links = soup.find_all("a", href=True)[:limit]

                for link in material_links:
                    try:
                        link_href = str(link.get("href", ""))
                        if link_href:
                            material_url = urljoin(url, link_href)
                            task = await self._extract_nsportal_material(material_url, subject)
                            if task:
                                tasks.append(task)
                    except Exception as e:
                        logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–∞: {e}")
                        continue

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–µ–¥–º–µ—Ç–∞ {subject}: {e}")

        return tasks

    async def _extract_nsportal_material(self, url: str, subject: str) -> EducationalContent | None:
        """–ò–∑–≤–ª–µ—á—å –º–∞—Ç–µ—Ä–∏–∞–ª —Å nsportal.ru."""
        try:
            response = await self._safe_get(url)
            if response and response.status == 200:
                html = await response.text()
                soup = BeautifulSoup(html, "html.parser")

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title_elem = soup.find("h1") or soup.find("title")
                title = title_elem.get_text().strip() if title_elem else "–ú–∞—Ç–µ—Ä–∏–∞–ª –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                content_elem = soup.find("div", class_="content") or soup.find("main")
                if not content_elem:
                    content_elem = soup.find("body")

                content = content_elem.get_text().strip() if content_elem else ""

                # –û—á–∏—â–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                content = re.sub(r"\s+", " ", content)
                content = content[:2000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É

                if len(content) > 100:  # –¢–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
                    return EducationalContent(
                        title=title,
                        content=content,
                        subject=subject,
                        difficulty="—Å—Ä–µ–¥–Ω—è—è",
                        source_url=url,
                        extracted_at=datetime.now(),
                        tags=[subject, "nsportal.ru"],
                    )

        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–∞ {url}: {e}")

        return None

    async def scrape_school203_content(self, limit: int = 30) -> list[EducationalContent]:
        """
        –ü–∞—Ä—Å–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å school203.spb.ru.

        Args:
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤.

        Returns:
            List[EducationalContent]: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤.
        """
        materials = []

        try:
            # –ü–∞—Ä—Å–∏–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
            for section in self.school203_config["sections"]:
                section_url = f"{self.school203_config['base_url']}/{section}"
                section_materials = await self._scrape_school203_section(
                    section_url, section, limit // len(self.school203_config["sections"])
                )
                materials.extend(section_materials)

                await asyncio.sleep(1)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ school203.spb.ru: {e}")

        return materials[:limit]

    async def _scrape_school203_section(
        self, url: str, section: str, limit: int
    ) -> list[EducationalContent]:
        """–ü–∞—Ä—Å–∏—Ç—å —Ä–∞–∑–¥–µ–ª school203.spb.ru."""
        materials = []

        try:
            response = await self._safe_get(url)
            if response and response.status == 200:
                html = await response.text()
                soup = BeautifulSoup(html, "html.parser")

                # –ò—â–µ–º —Å—Ç–∞—Ç—å–∏ –∏ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
                articles = soup.find_all(
                    ["article", "div"], class_=re.compile(r"(article|post|content)")
                )[:limit]

                for article in articles:
                    try:
                        material = await self._extract_school203_material(article, section)
                        if material:
                            materials.append(material)
                    except Exception as e:
                        logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏: {e}")
                        continue

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–∞–∑–¥–µ–ª–∞ {section}: {e}")

        return materials

    async def _extract_school203_material(
        self, article_elem, section: str
    ) -> EducationalContent | None:
        """–ò–∑–≤–ª–µ—á—å –º–∞—Ç–µ—Ä–∏–∞–ª –∏–∑ —Å—Ç–∞—Ç—å–∏ school203.spb.ru."""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title_elem = article_elem.find(["h1", "h2", "h3", "h4"])
            title = title_elem.get_text().strip() if title_elem else "–ú–∞—Ç–µ—Ä–∏–∞–ª –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            content_elem = article_elem.find("div", class_=re.compile(r"(content|text|body)"))
            if not content_elem:
                content_elem = article_elem

            content = content_elem.get_text().strip()
            content = re.sub(r"\s+", " ", content)
            content = content[:2000]

            if len(content) > 50:
                return EducationalContent(
                    title=title,
                    content=content,
                    subject=section,
                    difficulty="—Å—Ä–µ–¥–Ω—è—è",
                    source_url="",  # URL –Ω–µ –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–µ–Ω
                    extracted_at=datetime.now(),
                    tags=[section, "school203.spb.ru"],
                )

        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–∞: {e}")

        return None

    async def scrape_news(self, days_back: int = 7) -> list[NewsItem]:
        """
        –ü–∞—Ä—Å–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ —Å –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∞–π—Ç–æ–≤.

        Args:
            days_back: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π.

        Returns:
            List[NewsItem]: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π.
        """
        news = []

        try:
            # –ü–∞—Ä—Å–∏–º –Ω–æ–≤–æ—Å—Ç–∏ —Å nsportal.ru
            nsportal_news = await self._scrape_nsportal_news(days_back)
            news.extend(nsportal_news)

            # –ü–∞—Ä—Å–∏–º –Ω–æ–≤–æ—Å—Ç–∏ —Å school203.spb.ru
            school203_news = await self._scrape_school203_news(days_back)
            news.extend(school203_news)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")

        return news

    async def _scrape_nsportal_news(self, days_back: int) -> list[NewsItem]:  # noqa: ARG002
        """–ü–∞—Ä—Å–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ —Å nsportal.ru."""
        news = []

        try:
            news_url = self.nsportal_config["news_url"]
            response = await self._safe_get(news_url)
            if response and response.status == 200:
                html = await response.text()
                soup = BeautifulSoup(html, "html.parser")

                # –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏
                news_items = soup.find_all(
                    ["article", "div"], class_=re.compile(r"(news|article|post)")
                )[:20]

                for item in news_items:
                    try:
                        news_item = await self._extract_news_item(item, "nsportal.ru")
                        if news_item:
                            news.append(news_item)
                    except Exception as e:
                        logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏: {e}")
                        continue

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π nsportal.ru: {e}")

        return news

    async def _scrape_school203_news(self, days_back: int) -> list[NewsItem]:  # noqa: ARG002
        """–ü–∞—Ä—Å–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ —Å school203.spb.ru."""
        news = []

        try:
            # –ò—â–µ–º —Ä–∞–∑–¥–µ–ª –Ω–æ–≤–æ—Å—Ç–µ–π
            news_url = f"{self.school203_config['base_url']}/novosti"
            response = await self._safe_get(news_url)
            if response and response.status == 200:
                html = await response.text()
                soup = BeautifulSoup(html, "html.parser")

                # –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏
                news_items = soup.find_all(
                    ["article", "div"], class_=re.compile(r"(news|article|post)")
                )[:15]

                for item in news_items:
                    try:
                        news_item = await self._extract_news_item(item, "school203.spb.ru")
                        if news_item:
                            news.append(news_item)
                    except Exception as e:
                        logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏: {e}")
                        continue

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π school203.spb.ru: {e}")

        return news

    async def _extract_news_item(self, item_elem, source: str) -> NewsItem | None:
        """–ò–∑–≤–ª–µ—á—å –Ω–æ–≤–æ—Å—Ç—å –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞."""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title_elem = item_elem.find(["h1", "h2", "h3", "h4", "a"])
            title = title_elem.get_text().strip() if title_elem else "–ù–æ–≤–æ—Å—Ç—å –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            content_elem = item_elem.find(
                ["p", "div"], class_=re.compile(r"(content|text|summary)")
            )
            if not content_elem:
                content_elem = item_elem

            content = content_elem.get_text().strip()
            content = re.sub(r"\s+", " ", content)
            content = content[:500]

            # –ò–∑–≤–ª–µ–∫–∞–µ–º URL
            url_elem = item_elem.find("a", href=True)
            url = (
                urljoin(
                    (
                        self.nsportal_config["base_url"]
                        if "nsportal" in source
                        else self.school203_config["base_url"]
                    ),
                    url_elem["href"],
                )
                if url_elem
                else ""
            )

            if len(content) > 20:
                return NewsItem(
                    title=title,
                    content=content,
                    url=url,
                    published_date=datetime.now(),  # –¢–æ—á–Ω—É—é –¥–∞—Ç—É —Å–ª–æ–∂–Ω–æ –∏–∑–≤–ª–µ—á—å
                    source=source,
                )

        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏: {e}")

        return None

    async def get_educational_knowledge_base(self) -> dict[str, list[EducationalContent]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –ø–æ –≤—Å–µ–º –ø—Ä–µ–¥–º–µ—Ç–∞–º.

        Returns:
            Dict[str, List[EducationalContent]]: –°–ª–æ–≤–∞—Ä—å —Å –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏ –ø–æ –ø—Ä–µ–¥–º–µ—Ç–∞–º.
        """
        knowledge_base: dict[str, list[EducationalContent]] = {}

        try:
            # –°–æ–±–∏—Ä–∞–µ–º –º–∞—Ç–µ—Ä–∏–∞–ª—ã —Å nsportal.ru
            nsportal_materials = await self.scrape_nsportal_tasks(100)

            # –°–æ–±–∏—Ä–∞–µ–º –º–∞—Ç–µ—Ä–∏–∞–ª—ã —Å school203.spb.ru
            school203_materials = await self.scrape_school203_content(50)

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
            all_materials = nsportal_materials + school203_materials

            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø—Ä–µ–¥–º–µ—Ç–∞–º
            for material in all_materials:
                subject = material.subject
                if subject not in knowledge_base:
                    knowledge_base[subject] = []
                knowledge_base[subject].append(material)

            logger.info(
                f"üìö –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ–±—Ä–∞–Ω–∞: {len(all_materials)} –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –ø–æ {len(knowledge_base)} –ø—Ä–µ–¥–º–µ—Ç–∞–º"
            )

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")

        return knowledge_base


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
_web_scraper = None


async def get_web_scraper() -> WebScraperService:
    """
    –ü–æ–ª—É—á–∏—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–µ–±-—Å–∞–π—Ç–æ–≤.

    Returns:
        WebScraperService: –≠–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞.
    """
    global _web_scraper

    if _web_scraper is None:
        _web_scraper = WebScraperService()
        await _web_scraper.start_session()

    return _web_scraper
