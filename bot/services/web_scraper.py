"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —É—á–µ–±–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤.

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Ä–µ–∞–ª–∏–∑—É–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–±–æ—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
—Å –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∞–π—Ç–æ–≤, –≤–∫–ª—é—á–∞—è –∑–∞–¥–∞—á–∏, –Ω–æ–≤–æ—Å—Ç–∏ –∏ —É—á–µ–±–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã.
"""

import asyncio
import re
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Set
from urllib.parse import urljoin, urlparse

import aiohttp
from bs4 import BeautifulSoup
from loguru import logger


@dataclass
class EducationalContent:
    """
    –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.

    Attributes:
        title (str): –ù–∞–∑–≤–∞–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞ –∏–ª–∏ –∑–∞–¥–∞—á–∏.
        content (str): –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞.
        subject (str): –£—á–µ–±–Ω—ã–π –ø—Ä–µ–¥–º–µ—Ç (–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞, —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –∏ —Ç.–¥.).
        difficulty (str): –£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (–ª–µ–≥–∫–∏–π, —Å—Ä–µ–¥–Ω–∏–π, —Å–ª–æ–∂–Ω—ã–π).
        source_url (str): URL –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–∞.
        extracted_at (datetime): –í—Ä–µ–º—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞.
        tags (List[str]): –¢–µ–≥–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –ø–æ–∏—Å–∫–∞.
    """

    title: str
    content: str
    subject: str
    difficulty: str
    source_url: str
    extracted_at: datetime
    tags: List[str]


@dataclass
class NewsItem:
    """
    –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π.

    Attributes:
        title (str): –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏.
        content (str): –¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏.
        url (str): –°—Å—ã–ª–∫–∞ –Ω–∞ –ø–æ–ª–Ω—É—é –≤–µ—Ä—Å–∏—é.
        published_date (Optional[datetime]): –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞).
        source (str): –ò—Å—Ç–æ—á–Ω–∏–∫ –Ω–æ–≤–æ—Å—Ç–∏ (nsportal.ru, school203.spb.ru –∏ —Ç.–¥.).
    """

    title: str
    content: str
    url: str
    published_date: Optional[datetime]
    source: str


class WebScraperService:
    """
    –°–µ—Ä–≤–∏—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∞–π—Ç–æ–≤.

    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–¥–∞—á, –Ω–æ–≤–æ—Å—Ç–µ–π –∏ —É—á–µ–±–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
    —Å nsportal.ru –∏ school203.spb.ru.
    """

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞."""
        self.session: Optional[aiohttp.ClientSession] = None
        self.user_agent = (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å–∞–π—Ç–æ–≤
        self.nsportal_config = {
            "base_url": "https://nsportal.ru",
            "library_url": "https://nsportal.ru/shkola",
            "news_url": "https://nsportal.ru/novosti",
            "subjects": [
                "matematika",
                "russkiy-yazyk",
                "literatura",
                "istoriya",
                "geografiya",
                "biologiya",
                "fizika",
                "khimiya",
                "informatika",
            ],
        }

        self.school203_config = {
            "base_url": "https://school203.spb.ru",
            "sections": [
                "uchebnaya-deyatelnost",
                "vospitatelnaya-rabota",
                "dopolnitelnoe-obrazovanie",
                "metodicheskaya-rabota",
            ],
        }

        logger.info("üåê WebScraperService –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—Ö–æ–¥."""
        await self.start_session()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—ã—Ö–æ–¥."""
        await self.close_session()

    async def start_session(self):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å HTTP —Å–µ—Å—Å–∏—é."""
        if not self.session:
            timeout = aiohttp.ClientTimeout(total=30)
            self.session = aiohttp.ClientSession(
                timeout=timeout, headers={"User-Agent": self.user_agent}
            )

    async def close_session(self):
        """–ó–∞–∫—Ä—ã—Ç—å HTTP —Å–µ—Å—Å–∏—é."""
        if self.session:
            await self.session.close()
            self.session = None

    async def scrape_nsportal_tasks(self, limit: int = 50) -> List[EducationalContent]:
        """
        –ü–∞—Ä—Å–∏—Ç—å –∑–∞–¥–∞—á–∏ —Å nsportal.ru.

        Args:
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è.

        Returns:
            List[EducationalContent]: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á.
        """
        tasks = []

        try:
            # –ü–∞—Ä—Å–∏–º –±–∏–±–ª–∏–æ—Ç–µ–∫—É –ø–æ –ø—Ä–µ–¥–º–µ—Ç–∞–º
            for subject in self.nsportal_config["subjects"]:
                subject_url = f"{self.nsportal_config['library_url']}/{subject}"
                subject_tasks = await self._scrape_nsportal_subject(
                    subject_url, subject, limit // len(self.nsportal_config["subjects"])
                )
                tasks.extend(subject_tasks)

                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                await asyncio.sleep(1)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ nsportal.ru: {e}")

        return tasks[:limit]

    async def _scrape_nsportal_subject(
        self, url: str, subject: str, limit: int
    ) -> List[EducationalContent]:
        """–ü–∞—Ä—Å–∏—Ç—å –∑–∞–¥–∞—á–∏ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –ø—Ä–µ–¥–º–µ—Ç—É —Å nsportal.ru."""
        tasks = []

        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, "html.parser")

                    # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
                    material_links = soup.find_all("a", href=True)[:limit]

                    for link in material_links:
                        try:
                            material_url = urljoin(url, link["href"])
                            task = await self._extract_nsportal_material(material_url, subject)
                            if task:
                                tasks.append(task)
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–∞: {e}")
                            continue

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–µ–¥–º–µ—Ç–∞ {subject}: {e}")

        return tasks

    async def _extract_nsportal_material(
        self, url: str, subject: str
    ) -> Optional[EducationalContent]:
        """–ò–∑–≤–ª–µ—á—å –º–∞—Ç–µ—Ä–∏–∞–ª —Å nsportal.ru."""
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, "html.parser")

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                    title_elem = soup.find("h1") or soup.find("title")
                    title = title_elem.get_text().strip() if title_elem else "–ú–∞—Ç–µ—Ä–∏–∞–ª –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                    content_elem = soup.find("div", class_="content") or soup.find("main")
                    if not content_elem:
                        content_elem = soup.find("body")

                    content = content_elem.get_text().strip() if content_elem else ""

                    # –û—á–∏—â–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                    content = re.sub(r"\s+", " ", content)
                    content = content[:2000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É

                    if len(content) > 100:  # –¢–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
                        return EducationalContent(
                            title=title,
                            content=content,
                            subject=subject,
                            difficulty="—Å—Ä–µ–¥–Ω—è—è",
                            source_url=url,
                            extracted_at=datetime.now(),
                            tags=[subject, "nsportal.ru"],
                        )

        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–∞ {url}: {e}")

        return None

    async def scrape_school203_content(self, limit: int = 30) -> List[EducationalContent]:
        """
        –ü–∞—Ä—Å–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å school203.spb.ru.

        Args:
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤.

        Returns:
            List[EducationalContent]: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤.
        """
        materials = []

        try:
            # –ü–∞—Ä—Å–∏–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
            for section in self.school203_config["sections"]:
                section_url = f"{self.school203_config['base_url']}/{section}"
                section_materials = await self._scrape_school203_section(
                    section_url, section, limit // len(self.school203_config["sections"])
                )
                materials.extend(section_materials)

                await asyncio.sleep(1)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ school203.spb.ru: {e}")

        return materials[:limit]

    async def _scrape_school203_section(
        self, url: str, section: str, limit: int
    ) -> List[EducationalContent]:
        """–ü–∞—Ä—Å–∏—Ç—å —Ä–∞–∑–¥–µ–ª school203.spb.ru."""
        materials = []

        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, "html.parser")

                    # –ò—â–µ–º —Å—Ç–∞—Ç—å–∏ –∏ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
                    articles = soup.find_all(
                        ["article", "div"], class_=re.compile(r"(article|post|content)")
                    )[:limit]

                    for article in articles:
                        try:
                            material = await self._extract_school203_material(article, section)
                            if material:
                                materials.append(material)
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏: {e}")
                            continue

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–∞–∑–¥–µ–ª–∞ {section}: {e}")

        return materials

    async def _extract_school203_material(
        self, article_elem, section: str
    ) -> Optional[EducationalContent]:
        """–ò–∑–≤–ª–µ—á—å –º–∞—Ç–µ—Ä–∏–∞–ª –∏–∑ —Å—Ç–∞—Ç—å–∏ school203.spb.ru."""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title_elem = article_elem.find(["h1", "h2", "h3", "h4"])
            title = title_elem.get_text().strip() if title_elem else "–ú–∞—Ç–µ—Ä–∏–∞–ª –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            content_elem = article_elem.find("div", class_=re.compile(r"(content|text|body)"))
            if not content_elem:
                content_elem = article_elem

            content = content_elem.get_text().strip()
            content = re.sub(r"\s+", " ", content)
            content = content[:2000]

            if len(content) > 50:
                return EducationalContent(
                    title=title,
                    content=content,
                    subject=section,
                    difficulty="—Å—Ä–µ–¥–Ω—è—è",
                    source_url="",  # URL –Ω–µ –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–µ–Ω
                    extracted_at=datetime.now(),
                    tags=[section, "school203.spb.ru"],
                )

        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–∞: {e}")

        return None

    async def scrape_news(self, days_back: int = 7) -> List[NewsItem]:
        """
        –ü–∞—Ä—Å–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ —Å –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∞–π—Ç–æ–≤.

        Args:
            days_back: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π.

        Returns:
            List[NewsItem]: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π.
        """
        news = []

        try:
            # –ü–∞—Ä—Å–∏–º –Ω–æ–≤–æ—Å—Ç–∏ —Å nsportal.ru
            nsportal_news = await self._scrape_nsportal_news(days_back)
            news.extend(nsportal_news)

            # –ü–∞—Ä—Å–∏–º –Ω–æ–≤–æ—Å—Ç–∏ —Å school203.spb.ru
            school203_news = await self._scrape_school203_news(days_back)
            news.extend(school203_news)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")

        return news

    async def _scrape_nsportal_news(self, days_back: int) -> List[NewsItem]:
        """–ü–∞—Ä—Å–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ —Å nsportal.ru."""
        news = []

        try:
            news_url = self.nsportal_config["news_url"]
            async with self.session.get(news_url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, "html.parser")

                    # –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏
                    news_items = soup.find_all(
                        ["article", "div"], class_=re.compile(r"(news|article|post)")
                    )[:20]

                    for item in news_items:
                        try:
                            news_item = await self._extract_news_item(item, "nsportal.ru")
                            if news_item:
                                news.append(news_item)
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏: {e}")
                            continue

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π nsportal.ru: {e}")

        return news

    async def _scrape_school203_news(self, days_back: int) -> List[NewsItem]:
        """–ü–∞—Ä—Å–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ —Å school203.spb.ru."""
        news = []

        try:
            # –ò—â–µ–º —Ä–∞–∑–¥–µ–ª –Ω–æ–≤–æ—Å—Ç–µ–π
            news_url = f"{self.school203_config['base_url']}/novosti"
            async with self.session.get(news_url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, "html.parser")

                    # –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏
                    news_items = soup.find_all(
                        ["article", "div"], class_=re.compile(r"(news|article|post)")
                    )[:15]

                    for item in news_items:
                        try:
                            news_item = await self._extract_news_item(item, "school203.spb.ru")
                            if news_item:
                                news.append(news_item)
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏: {e}")
                            continue

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π school203.spb.ru: {e}")

        return news

    async def _extract_news_item(self, item_elem, source: str) -> Optional[NewsItem]:
        """–ò–∑–≤–ª–µ—á—å –Ω–æ–≤–æ—Å—Ç—å –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞."""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title_elem = item_elem.find(["h1", "h2", "h3", "h4", "a"])
            title = title_elem.get_text().strip() if title_elem else "–ù–æ–≤–æ—Å—Ç—å –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            content_elem = item_elem.find(
                ["p", "div"], class_=re.compile(r"(content|text|summary)")
            )
            if not content_elem:
                content_elem = item_elem

            content = content_elem.get_text().strip()
            content = re.sub(r"\s+", " ", content)
            content = content[:500]

            # –ò–∑–≤–ª–µ–∫–∞–µ–º URL
            url_elem = item_elem.find("a", href=True)
            url = (
                urljoin(
                    (
                        self.nsportal_config["base_url"]
                        if "nsportal" in source
                        else self.school203_config["base_url"]
                    ),
                    url_elem["href"],
                )
                if url_elem
                else ""
            )

            if len(content) > 20:
                return NewsItem(
                    title=title,
                    content=content,
                    url=url,
                    published_date=datetime.now(),  # –¢–æ—á–Ω—É—é –¥–∞—Ç—É —Å–ª–æ–∂–Ω–æ –∏–∑–≤–ª–µ—á—å
                    source=source,
                )

        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏: {e}")

        return None

    async def get_educational_knowledge_base(self) -> Dict[str, List[EducationalContent]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –ø–æ –≤—Å–µ–º –ø—Ä–µ–¥–º–µ—Ç–∞–º.

        Returns:
            Dict[str, List[EducationalContent]]: –°–ª–æ–≤–∞—Ä—å —Å –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏ –ø–æ –ø—Ä–µ–¥–º–µ—Ç–∞–º.
        """
        knowledge_base = {}

        try:
            # –°–æ–±–∏—Ä–∞–µ–º –º–∞—Ç–µ—Ä–∏–∞–ª—ã —Å nsportal.ru
            nsportal_materials = await self.scrape_nsportal_tasks(100)

            # –°–æ–±–∏—Ä–∞–µ–º –º–∞—Ç–µ—Ä–∏–∞–ª—ã —Å school203.spb.ru
            school203_materials = await self.scrape_school203_content(50)

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
            all_materials = nsportal_materials + school203_materials

            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø—Ä–µ–¥–º–µ—Ç–∞–º
            for material in all_materials:
                subject = material.subject
                if subject not in knowledge_base:
                    knowledge_base[subject] = []
                knowledge_base[subject].append(material)

            logger.info(
                f"üìö –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ–±—Ä–∞–Ω–∞: {len(all_materials)} –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –ø–æ {len(knowledge_base)} –ø—Ä–µ–¥–º–µ—Ç–∞–º"
            )

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")

        return knowledge_base


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
_web_scraper = None


async def get_web_scraper() -> WebScraperService:
    """
    –ü–æ–ª—É—á–∏—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–µ–±-—Å–∞–π—Ç–æ–≤.

    Returns:
        WebScraperService: –≠–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞.
    """
    global _web_scraper

    if _web_scraper is None:
        _web_scraper = WebScraperService()
        await _web_scraper.start_session()

    return _web_scraper
