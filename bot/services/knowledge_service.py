"""
–°–µ—Ä–≤–∏—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Å AI.

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–µ–±-–ø–∞—Ä—Å–∏–Ω–≥ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ —Å AI –¥–ª—è
–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –∏ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ —à–∫–æ–ª—å–Ω—ã–º –ø—Ä–µ–¥–º–µ—Ç–∞–º.
–¢–µ–ø–µ—Ä—å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Wikipedia API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
"""

import json
import os
import re
from datetime import datetime, timedelta
from urllib.parse import quote

import httpx
from loguru import logger

from bot.services.cache_service import cache_service
from bot.services.rag import QueryExpander, ResultReranker, SemanticCache, VectorSearchService
from bot.services.web_scraper import EducationalContent, WebScraperService


class KnowledgeService:
    """
    –°–µ—Ä–≤–∏—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π.

    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥ –≤–µ–±-—Å–∞–π—Ç–æ–≤ —Å AI –¥–ª—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
    –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤—Å–µ–º —à–∫–æ–ª—å–Ω—ã–º –ø—Ä–µ–¥–º–µ—Ç–∞–º.
    """

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ –∑–Ω–∞–Ω–∏–π."""
        self.knowledge_base: dict[str, list[EducationalContent]] = {}
        self.last_update: datetime | None = None
        self.update_interval = timedelta(days=7)  # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞–∑ –≤ –Ω–µ–¥–µ–ª—é
        self.auto_update_enabled = os.getenv("KNOWLEDGE_AUTO_UPDATE", "false").lower() == "true"

        # Wikipedia API (–ë–ï–ó –∫–ª—é—á–∞ - –æ—Ç–∫—Ä—ã—Ç—ã–π API)
        self.wikipedia_url = "https://ru.wikipedia.org/w/api.php"
        self.wikipedia_timeout = httpx.Timeout(10.0, connect=5.0)

        # RAG –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.query_expander = QueryExpander()
        self.reranker = ResultReranker()
        self.semantic_cache = SemanticCache(ttl_hours=24)
        self.vector_search = VectorSearchService()

        # –ó–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –¥–µ—Ç–µ–π (–¢–û–õ–¨–ö–û –æ–ø–∞—Å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –ù–ï –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π)
        # "–≤–æ–π–Ω–∞", "—Å–º–µ—Ä—Ç—å" –∏ —Ç.–¥. - —ç—Ç–æ —É—á–µ–±–Ω—ã–µ —Ç–µ–º—ã –∏—Å—Ç–æ—Ä–∏–∏, –∏—Ö –ù–ï –±–ª–æ–∫–∏—Ä—É–µ–º
        self.forbidden_topics = {
            # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –æ–ø–∞—Å–Ω—ã—Ö –≤–µ—â–µ–π
            "–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å –±–æ–º–±—É",
            "–∫–∞–∫ –∏–∑–≥–æ—Ç–æ–≤–∏—Ç—å –≤–∑—Ä—ã–≤—á–∞—Ç–∫—É",
            "—Ä–µ—Ü–µ–ø—Ç –Ω–∞—Ä–∫–æ—Ç–∏–∫",
            # –ü—Ä–∏–∑—ã–≤—ã –∫ –Ω–∞—Å–∏–ª–∏—é –∏ —Å—É–∏—Ü–∏–¥—É
            "—Å–ø–æ—Å–æ–±—ã —Å–∞–º–æ—É–±–∏–π—Å—Ç–≤–∞",
            "–∫–∞–∫ –ø–æ–∫–æ–Ω—á–∏—Ç—å —Å —Å–æ–±–æ–π",
            "–ø—Ä–∏–∑—ã–≤ –∫ –Ω–∞—Å–∏–ª–∏—é",
            # –≠–∫—Å—Ç—Ä–µ–º–∏–∑–º –∏ —Ç–µ—Ä—Ä–æ—Ä–∏–∑–º
            "–≤—Å—Ç—É–ø–∏—Ç—å –≤ –∏–≥–∏–ª",
            "—Ç–µ—Ä—Ä–æ—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –≤–µ—Ä–±—É–µ—Ç",
            # –í–∑—Ä–æ—Å–ª—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
            "–ø–æ—Ä–Ω–æ–≥—Ä–∞—Ñ",
            "—ç—Ä–æ—Ç–∏—á–µ—Å–∫",
        }

        logger.info(
            f"üìö KnowledgeService –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (RAG: ON, –∞–≤—Ç–æ-–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: {'–í–ö–õ' if self.auto_update_enabled else '–í–´–ö–õ'})"
        )

    async def get_knowledge_for_subject(
        self, subject: str, query: str = ""
    ) -> list[EducationalContent]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∑–Ω–∞–Ω–∏—è –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –ø—Ä–µ–¥–º–µ—Ç—É.

        Args:
            subject: –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–º–µ—Ç–∞.
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.

        Returns:
            List[EducationalContent]: –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤.
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        if self._should_update_knowledge_base():
            await self.update_knowledge_base()

        # –ü–æ–ª—É—á–∞–µ–º –º–∞—Ç–µ—Ä–∏–∞–ª—ã –ø–æ –ø—Ä–µ–¥–º–µ—Ç—É
        subject_materials = self.knowledge_base.get(subject, [])

        # –ï—Å–ª–∏ –µ—Å—Ç—å –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å, —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if query:
            query_lower = query.lower()
            subject_materials = [
                material
                for material in subject_materials
                if query_lower in material.title.lower() or query_lower in material.content.lower()
            ]

        return subject_materials

    async def enhanced_search(
        self,
        user_question: str,
        user_age: int | None = None,
        top_k: int = 3,
        use_wikipedia: bool = True,
    ) -> list[EducationalContent]:
        """
        –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å RAG –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏.
        –ü—Ä–∏ –ø—É—Å—Ç–æ–π –±–∞–∑–µ –∏ use_wikipedia=True –ø–æ–¥—Ç—è–≥–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ Wikipedia.

        Args:
            user_question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_age: –í–æ–∑—Ä–∞—Å—Ç –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            use_wikipedia: –ü–æ–¥—Ç—è–Ω—É—Ç—å Wikipedia –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ –±–∞–∑—ã

        Returns:
            –¢–æ–ø-K –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        cached_result = await self.semantic_cache.get(user_question)
        if cached_result:
            logger.debug(f"üìö Semantic cache hit: {user_question[:50]}")
            return cached_result

        # 1. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ (semantic)
        vector_results = await self.vector_search.search(user_question, top_k=5)

        # 2. Keyword –ø–æ–∏—Å–∫
        expanded_query = self.query_expander.expand(user_question)
        logger.debug(f"üìö Expanded query: {expanded_query}")
        query_variations = self.query_expander.generate_variations(user_question)
        keyword_results = []
        for variation in query_variations:
            results = await self.get_helpful_content(variation, user_age)
            keyword_results.extend(results)

        all_results = vector_results + keyword_results
        unique_results = self._deduplicate_results(all_results)

        # 3. Wikipedia fallback; –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ ‚Äî –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –¥–ª—è –±—É–¥—É—â–µ–≥–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
        if not unique_results and use_wikipedia:
            wiki_content = await self.get_wikipedia_educational(user_question, user_age)
            if wiki_content:
                unique_results = [wiki_content]
                await self.vector_search.index_content(wiki_content)

        ranked_results = self.reranker.rerank(user_question, unique_results, user_age, top_k=top_k)

        if ranked_results:
            await self.semantic_cache.set(user_question, ranked_results)

        return ranked_results

    async def get_helpful_content(
        self,
        user_question: str,
        user_age: int | None = None,  # noqa: ARG002
    ) -> list[EducationalContent]:
        """
        –ù–∞–π—Ç–∏ –ø–æ–ª–µ–∑–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

        Args:
            user_question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
            user_age: –í–æ–∑—Ä–∞—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏.

        Returns:
            List[EducationalContent]: –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤.
        """
        # –û–±–Ω–æ–≤–ª—è–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if self._should_update_knowledge_base():
            await self.update_knowledge_base()

        relevant_materials = []
        question_lower = user_question.lower()

        # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –≤–æ –≤—Å–µ—Ö –ø—Ä–µ–¥–º–µ—Ç–∞—Ö
        for subject, materials in self.knowledge_base.items():
            for material in materials:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é
                if (
                    question_lower in material.title.lower()
                    or question_lower in material.content.lower()
                    or self._is_question_related_to_subject(question_lower, subject)
                ):
                    relevant_materials.append(material)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–ø—Ä–æ—Å—Ç–µ–π—à–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º)
        relevant_materials.sort(key=lambda x: len(x.content), reverse=True)

        return relevant_materials[:5]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-5 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤

    def _is_question_related_to_subject(self, question: str, subject: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —Å–≤—è–∑–∞–Ω –ª–∏ –≤–æ–ø—Ä–æ—Å —Å –ø—Ä–µ–¥–º–µ—Ç–æ–º.

        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ.
            subject: –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–º–µ—Ç–∞.

        Returns:
            bool: True –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —Å–≤—è–∑–∞–Ω —Å –ø—Ä–µ–¥–º–µ—Ç–æ–º.
        """
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–º–µ—Ç–∞
        subject_keywords = {
            "matematika": [
                "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞",
                "—á–∏—Å–ª–æ",
                "—Ä–µ—à–∏—Ç—å",
                "–∑–∞–¥–∞—á–∞",
                "—É—Ä–∞–≤–Ω–µ–Ω–∏–µ",
                "—Ñ–æ—Ä–º—É–ª–∞",
                "—Å—á–∏—Ç–∞—Ç—å",
                "–ø–ª—é—Å",
                "–º–∏–Ω—É—Å",
                "—É–º–Ω–æ–∂–∏—Ç—å",
                "–¥–µ–ª–∏—Ç—å",
                "–¥—Ä–æ–±—å",
                "–ø—Ä–æ—Ü–µ–Ω—Ç",
                "–≥–µ–æ–º–µ—Ç—Ä–∏—è",
                "–∞–ª–≥–µ–±—Ä–∞",
            ],
            "russkiy-yazyk": [
                "—Ä—É—Å—Å–∫–∏–π",
                "—è–∑—ã–∫",
                "—Å–ª–æ–≤–æ",
                "–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ",
                "–≥—Ä–∞–º–º–∞—Ç–∏–∫–∞",
                "–æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—è",
                "–ø—É–Ω–∫—Ç—É–∞—Ü–∏—è",
                "—á–∞—Å—Ç—å —Ä–µ—á–∏",
                "—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ",
                "–ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ",
                "–≥–ª–∞–≥–æ–ª",
                "–Ω–∞–ø–∏—Å–∞—Ç—å",
                "—Å–æ—á–∏–Ω–µ–Ω–∏–µ",
                "–∏–∑–ª–æ–∂–µ–Ω–∏–µ",
            ],
            "literatura": [
                "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞",
                "–∫–Ω–∏–≥–∞",
                "–ø–∏—Å–∞—Ç–µ–ª—å",
                "–ø–æ—ç—Ç",
                "—Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ",
                "—Ä–∞—Å—Å–∫–∞–∑",
                "—Ä–æ–º–∞–Ω",
                "–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ",
                "–∞–≤—Ç–æ—Ä",
                "–≥–µ—Ä–æ–π",
                "—Å—é–∂–µ—Ç",
                "—Ç–µ–º–∞",
                "–∏–¥–µ—è",
            ],
            "istoriya": [
                "–∏—Å—Ç–æ—Ä–∏—è",
                "–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π",
                "–≤–æ–π–Ω–∞",
                "—Ä–µ–≤–æ–ª—é—Ü–∏—è",
                "—Ü–∞—Ä—å",
                "–∏–º–ø–µ—Ä–∞—Ç–æ—Ä",
                "–¥—Ä–µ–≤–Ω–∏–π",
                "—Å—Ä–µ–¥–Ω–∏–µ –≤–µ–∫–∞",
                "—Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç—å",
                "–¥–∞—Ç–∞",
                "—Å–æ–±—ã—Ç–∏–µ",
                "–ø–µ—Ä—Å–æ–Ω–∞",
            ],
            "geografiya": [
                "–≥–µ–æ–≥—Ä–∞—Ñ–∏—è",
                "—Å—Ç—Ä–∞–Ω–∞",
                "–≥–æ—Ä–æ–¥",
                "—Å—Ç–æ–ª–∏—Ü–∞",
                "–º–∞—Ç–µ—Ä–∏–∫",
                "–æ–∫–µ–∞–Ω",
                "—Ä–µ–∫–∞",
                "–≥–æ—Ä–∞",
                "–∫–ª–∏–º–∞—Ç",
                "–Ω–∞—Å–µ–ª–µ–Ω–∏–µ",
                "–∫–∞—Ä—Ç–∞",
                "–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã",
            ],
            "biologiya": [
                "–±–∏–æ–ª–æ–≥–∏—è",
                "–∂–∏–≤–æ—Ç–Ω–æ–µ",
                "—Ä–∞—Å—Ç–µ–Ω–∏–µ",
                "–∫–ª–µ—Ç–∫–∞",
                "–æ—Ä–≥–∞–Ω",
                "—Å–∏—Å—Ç–µ–º–∞",
                "—Ä–∞–∑–º–Ω–æ–∂–µ–Ω–∏–µ",
                "—ç–≤–æ–ª—é—Ü–∏—è",
                "—ç–∫–æ—Å–∏—Å—Ç–µ–º–∞",
                "–ø—Ä–∏—Ä–æ–¥–∞",
                "—á–µ–ª–æ–≤–µ–∫",
                "–∞–Ω–∞—Ç–æ–º–∏—è",
            ],
            "fizika": [
                "—Ñ–∏–∑–∏–∫–∞",
                "—Å–∏–ª–∞",
                "—ç–Ω–µ—Ä–≥–∏—è",
                "—Å–∫–æ—Ä–æ—Å—Ç—å",
                "–º–∞—Å—Å–∞",
                "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞",
                "–¥–∞–≤–ª–µ–Ω–∏–µ",
                "—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ",
                "–º–∞–≥–Ω–µ—Ç–∏–∑–º",
                "—Å–≤–µ—Ç",
                "–∑–≤—É–∫",
                "–º–µ—Ö–∞–Ω–∏–∫–∞",
            ],
            "khimiya": [
                "—Ö–∏–º–∏—è",
                "—Ö–∏–º–∏—á–µ—Å–∫–∏–π",
                "—ç–ª–µ–º–µ–Ω—Ç",
                "—Ä–µ–∞–∫—Ü–∏—è",
                "–≤–µ—â–µ—Å—Ç–≤–æ",
                "–º–æ–ª–µ–∫—É–ª–∞",
                "–∞—Ç–æ–º",
                "–ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è",
                "—Ç–∞–±–ª–∏—Ü–∞",
                "–º–µ–Ω–¥–µ–ª–µ–µ–≤–∞",
                "–º–µ–Ω–¥–µ–ª–µ–µ–≤",
                "–ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è —Ç–∞–±–ª–∏—Ü–∞",
                "—Ç–∞–±–ª–∏—Ü–∞ –º–µ–Ω–¥–µ–ª–µ–µ–≤–∞",
                "–∫–∏—Å–ª–æ—Ç–∞",
                "—â–µ–ª–æ—á—å",
                "—Å–æ–ª—å",
            ],
            "informatika": [
                "–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞",
                "–∫–æ–º–ø—å—é—Ç–µ—Ä",
                "–ø—Ä–æ–≥—Ä–∞–º–º–∞",
                "–∞–ª–≥–æ—Ä–∏—Ç–º",
                "–∫–æ–¥",
                "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ",
                "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç",
                "—Å–∞–π—Ç",
                "–¥–∞–Ω–Ω—ã–µ",
                "—Ñ–∞–π–ª",
                "—Å–∏—Å—Ç–µ–º–∞",
            ],
        }

        keywords = subject_keywords.get(subject, [])
        return any(keyword in question for keyword in keywords)

    def _should_update_knowledge_base(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π."""
        # –ï—Å–ª–∏ –∞–≤—Ç–æ-–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º False
        if not self.auto_update_enabled:
            return False

        if not self.last_update:
            return True

        time_diff = datetime.now() - self.last_update
        return bool(time_diff > self.update_interval)

    async def update_knowledge_base(self):
        """–û–±–Ω–æ–≤–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ –≤–µ–±-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""
        try:
            logger.info("üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")

            async with WebScraperService() as scraper:
                # –°–æ–±–∏—Ä–∞–µ–º –º–∞—Ç–µ—Ä–∏–∞–ª—ã —Å nsportal.ru
                nsportal_materials = await scraper.scrape_nsportal_tasks(100)

                # –°–æ–±–∏—Ä–∞–µ–º –º–∞—Ç–µ—Ä–∏–∞–ª—ã —Å school203.spb.ru
                school203_materials = await scraper.scrape_school203_content(50)

                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
                all_materials = nsportal_materials + school203_materials

                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø—Ä–µ–¥–º–µ—Ç–∞–º
                self.knowledge_base = {}
                for material in all_materials:
                    subject = material.subject
                    if subject not in self.knowledge_base:
                        self.knowledge_base[subject] = []
                    self.knowledge_base[subject].append(material)

                self.last_update = datetime.now()

                logger.info(
                    f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –æ–±–Ω–æ–≤–ª–µ–Ω–∞: {len(all_materials)} –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –ø–æ {len(self.knowledge_base)} –ø—Ä–µ–¥–º–µ—Ç–∞–º"
                )

                # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ pgvector –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
                indexed = 0
                for material in all_materials:
                    try:
                        if await self.vector_search.index_content(material):
                            indexed += 1
                    except Exception as idx_err:
                        logger.warning(
                            f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª ¬´{material.title[:50]}¬ª: {idx_err}"
                        )
                if indexed:
                    logger.info(f"üìö –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ –≤ –≤–µ–∫—Ç–æ—Ä—ã: {indexed} –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")

    def get_knowledge_stats(self) -> dict[str, int]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.

        Returns:
            Dict[str, int]: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–µ–¥–º–µ—Ç–∞–º.
        """
        stats = {}
        for subject, materials in self.knowledge_base.items():
            stats[subject] = len(materials)

        return stats

    @staticmethod
    def _paragraphize_snippet(text: str, max_chars: int = 1000) -> str:
        """–†–∞–∑–±–∏—Ç—å —Å–ø–ª–æ—à–Ω–æ–π —Ç–µ–∫—Å—Ç –Ω–∞ –∞–±–∑–∞—Ü—ã (–ø–æ 1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –Ω–µ –∫–æ–ø–∏—Ä–æ–≤–∞–ª–∞ —Å—Ç–µ–Ω—É —Ç–µ–∫—Å—Ç–∞."""
        if not text or len(text) > max_chars:
            text = (text or "")[:max_chars]
        if "\n\n" in text:
            return text.strip()
        sentences = re.split(r"(?<=[.!?])\s+", text)
        sentences = [s.strip() for s in sentences if s.strip()]
        if len(sentences) <= 1:
            return text.strip()
        parts = []
        buf = []
        for sent in sentences:
            buf.append(sent)
            if len(buf) >= 2:
                parts.append(" ".join(buf))
                buf = []
        if buf:
            parts.append(" ".join(buf))
        result = "\n\n".join(parts)
        return result[:max_chars] if len(result) > max_chars else result

    def format_knowledge_for_ai(self, materials: list[EducationalContent]) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª—ã –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ AI.

        Args:
            materials: –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤.

        Returns:
            str: –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è AI.
        """
        if not materials:
            return ""

        formatted_content = "\n\nüìö –†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –ú–ê–¢–ï–†–ò–ê–õ–´ –ò–ó –û–ë–†–ê–ó–û–í–ê–¢–ï–õ–¨–ù–´–• –ò–°–¢–û–ß–ù–ò–ö–û–í:\n"

        content_limit = 1000
        for i, material in enumerate(materials[:3], 1):  # –ë–µ—Ä–µ–º —Ç–æ–ø-3 –º–∞—Ç–µ—Ä–∏–∞–ª–∞
            formatted_content += f"\n{i}. {material.title}\n"
            formatted_content += f"   –ü—Ä–µ–¥–º–µ—Ç: {material.subject}\n"
            raw = material.content[:content_limit]
            content_snippet = self._paragraphize_snippet(raw) if raw else ""
            suffix = "..." if len(material.content) > content_limit else ""
            formatted_content += f"   –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {content_snippet}{suffix}\n"
            formatted_content += f"   –ò—Å—Ç–æ—á–Ω–∏–∫: {material.source_url}\n"

        formatted_content += (
            "\n\n–ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞. "
            "–ù–µ —É–ø–æ–º–∏–Ω–∞–π Wikipedia –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –æ—Ç–≤–µ—Ç–µ."
        )

        return formatted_content

    async def _wikipedia_search_title(self, topic: str) -> str | None:
        """
        –ù–∞–π—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ –ø–æ–∏—Å–∫–æ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É (fallback –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Ç–æ—á–Ω–æ–≥–æ titles).
        """
        try:
            params = {
                "action": "query",
                "list": "search",
                "srsearch": topic,
                "srlimit": 1,
                "format": "json",
            }
            headers = {
                "User-Agent": "PandaPal/1.0 (Educational Bot; contact@pandapal.ru)",
                "Accept": "application/json",
            }
            async with httpx.AsyncClient(timeout=self.wikipedia_timeout, headers=headers) as client:
                response = await client.get(self.wikipedia_url, params=params)
                response.raise_for_status()
                data = response.json()
            search = data.get("query", {}).get("search", [])
            if search:
                return search[0].get("title")
        except Exception as e:
            logger.debug(f"Wikipedia search fallback –¥–ª—è '{topic}': {e}")
        return None

    async def get_wikipedia_summary(
        self, topic: str, user_age: int | None = None, max_length: int = 500
    ) -> tuple[str, str] | None:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–µ–º—ã –∏–∑ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞.
        –ë–ï–ó –∫–ª—é—á–∞ - –æ—Ç–∫—Ä—ã—Ç—ã–π API, —Ä–∞–±–æ—Ç–∞–µ—Ç –∏–∑ –†–æ—Å—Å–∏–∏.
        –ü—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ —Ç–æ—á–Ω–æ–º—É titles –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–∏—Å–∫ (list=search).

        Args:
            topic: –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã –¥–ª—è –ø–æ–∏—Å–∫–∞.
            user_age: –í–æ–∑—Ä–∞—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ (—Å–∏–º–≤–æ–ª–æ–≤).

        Returns:
            (extract, title) –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ.
        """
        if not topic or not topic.strip():
            return None

        topic_normalized = topic.strip().lower()
        cache_key = f"wikipedia:{topic_normalized}:{user_age or 'all'}"

        cached = await cache_service.get(cache_key)
        if cached:
            try:
                obj = json.loads(cached)
                logger.debug(f"üìö –ö—ç—à –ø–æ–ø–∞–¥–∞–Ω–∏–µ –¥–ª—è —Ç–µ–º—ã: {topic}")
                return (obj["e"], obj["t"])
            except (json.JSONDecodeError, KeyError):
                pass

        headers = {
            "User-Agent": "PandaPal/1.0 (Educational Bot; contact@pandapal.ru)",
            "Accept": "application/json",
        }

        try:
            params = {
                "action": "query",
                "prop": "extracts",
                "exintro": "1",
                "explaintext": "1",
                "titles": topic,
                "format": "json",
            }
            async with httpx.AsyncClient(timeout=self.wikipedia_timeout, headers=headers) as client:
                response = await client.get(self.wikipedia_url, params=params)
                response.raise_for_status()
                data = response.json()

            pages = data.get("query", {}).get("pages", {})
            if not pages:
                logger.debug(f"üìö –°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–ª—è '{topic}'")
                return None

            page = list(pages.values())[0]
            title = page.get("title", topic)

            if page.get("missing") or page.get("invalid"):
                found_title = await self._wikipedia_search_title(topic)
                if found_title:
                    async with httpx.AsyncClient(
                        timeout=self.wikipedia_timeout, headers=headers
                    ) as client:
                        resp = await client.get(
                            self.wikipedia_url,
                            params={
                                "action": "query",
                                "prop": "extracts",
                                "exintro": "1",
                                "explaintext": "1",
                                "titles": found_title,
                                "format": "json",
                            },
                        )
                        resp.raise_for_status()
                        data = resp.json()
                    pages = data.get("query", {}).get("pages", {})
                    if not pages:
                        return None
                    page = list(pages.values())[0]
                    title = page.get("title", found_title)
                else:
                    logger.debug(f"üìö –°—Ç—Ä–∞–Ω–∏—Ü–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏ –ø–æ–∏—Å–∫ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è '{topic}'")
                    return None

            extract = page.get("extract", "").strip()
            if not extract:
                found_title = await self._wikipedia_search_title(topic)
                if found_title and found_title != title:
                    async with httpx.AsyncClient(
                        timeout=self.wikipedia_timeout, headers=headers
                    ) as client:
                        resp = await client.get(
                            self.wikipedia_url,
                            params={
                                "action": "query",
                                "prop": "extracts",
                                "exintro": "1",
                                "explaintext": "1",
                                "titles": found_title,
                                "format": "json",
                            },
                        )
                        resp.raise_for_status()
                        data = resp.json()
                    pages = data.get("query", {}).get("pages", {})
                    if pages:
                        page = list(pages.values())[0]
                        title = page.get("title", found_title)
                        extract = page.get("extract", "").strip()
            if not extract:
                logger.debug(f"üìö –ü—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è '{topic}'")
                return None

            if self._contains_forbidden_content(extract):
                logger.warning(f"‚ö†Ô∏è –ó–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –æ–±–Ω–∞—Ä—É–∂–µ–Ω –¥–ª—è '{topic}'")
                return None

            extract = self._adapt_content_for_children(extract, user_age)
            if len(extract) > max_length:
                sentences = re.split(r"([.!?]\s+)", extract[: max_length + 100])
                extract = "".join(sentences[:-2]) if len(sentences) > 2 else extract[:max_length]
                extract = extract.strip() + "..."

            await cache_service.set(cache_key, json.dumps({"e": extract, "t": title}), ttl=86400)
            logger.debug(
                f"‚úÖ –ü–æ–ª—É—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è '{topic}' ({len(extract)} —Å–∏–º–≤–æ–ª–æ–≤, –≤–æ–∑—Ä–∞—Å—Ç: {user_age or 'all'})"
            )
            return (extract, title)

        except httpx.TimeoutException:
            logger.warning(f"‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è '{topic}'")
            return None
        except httpx.HTTPStatusError as e:
            logger.warning(f"‚ö†Ô∏è HTTP –æ—à–∏–±–∫–∞ {e.response.status_code} –¥–ª—è '{topic}'")
            return None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è '{topic}': {e}")
            return None

    def _contains_forbidden_content(self, text: str) -> bool:  # noqa: ARG002
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç.
        –û—Ç–∫–ª—é—á–µ–Ω–æ: —Å–≤–æ–±–æ–¥–∞ –º–æ–¥–µ–ª–∏, —Å–∞–º–∞ –∑–Ω–∞–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã.
        """
        return False

    def _adapt_content_for_children(self, text: str, user_age: int | None = None) -> str:
        """
        –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –¥–µ—Ç–µ–π: —É–ø—Ä–æ—â–µ–Ω–∏–µ, —É–¥–∞–ª–µ–Ω–∏–µ —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤.

        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç.
            user_age: –í–æ–∑—Ä–∞—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

        Returns:
            str: –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.
        """
        # –£–¥–∞–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–æ–º–µ—Ç–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        text = re.sub(r"\[–ø—Ä–∏–º–µ—á–∞–Ω–∏–µ \d+\]", "", text, flags=re.IGNORECASE)
        text = re.sub(r"\[–∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–µ —É–∫–∞–∑–∞–Ω \d+ –¥–Ω–µ–π?\]", "", text, flags=re.IGNORECASE)
        text = re.sub(r"\[–∫–æ–≥–¥–∞\?\]", "", text, flags=re.IGNORECASE)

        # –£–ø—Ä–æ—â–∞–µ–º —Å–ª–æ–∂–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –º–ª–∞–¥—à–∏—Ö –∫–ª–∞—Å—Å–æ–≤
        if user_age and user_age <= 10:
            # –ó–∞–º–µ–Ω—è–µ–º —Å–ª–æ–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–µ
            replacements = {
                r"\b–æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è\b": "–ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç",
                r"\b—è–≤–ª—è–µ—Ç—Å—è\b": "—ç—Ç–æ",
                r"\b–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π\b": "—ç—Ç–æ",
                r"\b—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É–µ—Ç—Å—è\b": "–æ—Ç–ª–∏—á–∞–µ—Ç—Å—è",
                r"\b–æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å\b": "–¥–µ–ª–∞—Ç—å",
            }
            for pattern, replacement in replacements.items():
                text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)

        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r"\s+", " ", text)
        text = text.strip()

        return text

    def _extract_topic_from_question(self, question: str) -> str | None:
        """
        –ò–∑–≤–ª–µ—á—å —Ç–µ–º—É –∏–∑ –≤–æ–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

        Returns:
            str: –ò–∑–≤–ª–µ—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞ –∏–ª–∏ None.
        """
        question_lower = question.lower().strip()

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–º—ã
        patterns = [
            r"—Å–ø–∏—Å–æ–∫\s+(.+?)(?:\?|\.|$)",
            r"—Ç–∞–±–ª–∏—Ü[–∞–µ—ã]?\s+(?:–∑–Ω–∞—á–µ–Ω–∏–π?\s+)?(.+?)(?:\?|\.|$)",
            r"–≤—Å–µ\s+–∑–Ω–∞—á–µ–Ω–∏—è\s+(.+?)(?:\?|\.|$)",
            r"—á—Ç–æ —Ç–∞–∫–æ–µ\s+(.+?)(?:\?|\.|$)",
            r"–∫—Ç–æ —Ç–∞–∫–æ–π\s+(.+?)(?:\?|\.|$)",
            r"–∫—Ç–æ —Ç–∞–∫–∞—è\s+(.+?)(?:\?|\.|$)",
            r"—Ä–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ\s+(.+?)(?:\?|\.|$)",
            r"—Ä–∞—Å—Å–∫–∞–∂–∏ –æ\s+(.+?)(?:\?|\.|$)",
            r"–æ–±—ä—è—Å–Ω–∏\s+(.+?)(?:\?|\.|$)",
            r"—á—Ç–æ –∑–Ω–∞—á–∏—Ç\s+(.+?)(?:\?|\.|$)",
            r"—á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç\s+(.+?)(?:\?|\.|$)",
        ]

        for pattern in patterns:
            match = re.search(pattern, question_lower)
            if match:
                topic = match.group(1).strip()
                # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–ª–æ–≤–∞
                topic = re.sub(r"\s+(—ç—Ç–æ|—Ç–∞–∫–æ–µ|—Ç–∞–∫–æ–π|—Ç–∞–∫–∞—è)\s*$", "", topic, flags=re.IGNORECASE)
                if len(topic) > 2 and len(topic) < 100:  # –†–∞–∑—É–º–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã
                    return topic

        # –ï—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ —Å–ª–æ–≤–∞ –≤–æ–ø—Ä–æ—Å–∞
        words = question.split()
        if len(words) >= 2:
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2-4 —Å–ª–æ–≤–∞
            topic = " ".join(words[: min(4, len(words))])
            # –£–±–∏—Ä–∞–µ–º –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
            topic = re.sub(
                r"^(—á—Ç–æ|–∫—Ç–æ|–∫–∞–∫|–≥–¥–µ|–∫–æ–≥–¥–∞|–ø–æ—á–µ–º—É|–∑–∞—á–µ–º)\s+", "", topic, flags=re.IGNORECASE
            )
            if len(topic) > 2 and len(topic) < 100:
                return topic

        return None

    async def get_wikipedia_context_for_question(
        self, question: str, user_age: int | None = None
    ) -> str | None:
        """
        –ü–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
            user_age: –í–æ–∑—Ä–∞—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

        Returns:
            str: –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–ª–∏ None.
        """
        topic = self._extract_topic_from_question(question)
        if not topic:
            return None
        result = await self.get_wikipedia_summary(topic, user_age, max_length=1200)
        return result[0] if result else None

    async def get_wikipedia_educational(
        self, question: str, user_age: int | None = None
    ) -> EducationalContent | None:
        """
        –ü–æ–ª—É—á–∏—Ç—å Wikipedia-–∫–æ–Ω—Ç–µ–Ω—Ç –≤ –≤–∏–¥–µ EducationalContent –¥–ª—è RAG (–∫–æ–≥–¥–∞ –±–∞–∑–∞ –ø—É—Å—Ç–∞).
        """
        topic = self._extract_topic_from_question(question)
        if not topic:
            return None
        result = await self.get_wikipedia_summary(topic, user_age, max_length=1200)
        if not result:
            return None
        extract, title = result
        url_title = quote(title.replace(" ", "_"), safe="")
        return EducationalContent(
            title=title,
            content=extract,
            subject="–æ–±—â–µ–µ",
            difficulty="—Å—Ä–µ–¥–Ω–∏–π",
            source_url=f"https://ru.wikipedia.org/wiki/{url_title}",
            extracted_at=datetime.now(),
            tags=["wikipedia"],
        )

    def _deduplicate_results(self, results: list) -> list:
        """–£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
        seen_urls = set()
        unique = []

        for result in results:
            url = getattr(result, "source_url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique.append(result)
            elif not url:
                # –ï—Å–ª–∏ –Ω–µ—Ç URL, –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ title
                title = getattr(result, "title", "")
                if title and title not in [getattr(r, "title", "") for r in unique]:
                    unique.append(result)

        return unique


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
_knowledge_service = None


def get_knowledge_service() -> KnowledgeService:
    """
    –ü–æ–ª—É—á–∏—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞ –∑–Ω–∞–Ω–∏–π.

    Returns:
        KnowledgeService: –≠–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞.
    """
    global _knowledge_service

    if _knowledge_service is None:
        _knowledge_service = KnowledgeService()

    return _knowledge_service
